{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done By: **Ryan Yeo**\n",
    "\n",
    "Class: **DAAA/FT/1B/01**\n",
    "\n",
    "Admin Num: **P2214452**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: 'datasets_cleaned'\n"
     ]
    }
   ],
   "source": [
    "# Create directory if it exists print err\n",
    "try:\n",
    "    os.mkdir('datasets_cleaned')\n",
    "except OSError as error:\n",
    "    print(error)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean employment dataset\n",
    "\n",
    "# Note:\n",
    "# We cannot use genfromtxt directly because it reads commas contained in strings(unlike csvreader and pd.read_csv) \n",
    "# To avoid that, we first read in all the data seperated by a newline before processing it\n",
    "\n",
    "dirtyData_17to19 = np.genfromtxt('datasets_src/employment/emp_17to19.csv', dtype=\"U64\",delimiter=\"\\n\")\n",
    "dirtyData_19to21 = np.genfromtxt('datasets_src/employment/emp_19to21.csv', dtype=\"U64\",delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert and clean the dataset so that it can be written to datasets_cleaned\n",
    "def cleanData(dirtyArr):\n",
    "    employ_arr = []\n",
    "    for i in dirtyArr:\n",
    "        _ = []\n",
    "        inQuotes = False\n",
    "        for j,n in enumerate(i):\n",
    "            if n=='\\\"':\n",
    "                # If opening quotes => True elif closing quotes => False\n",
    "                inQuotes=not inQuotes\n",
    "            if n==',' and inQuotes:\n",
    "                # If it's used in a string, change it to a backtick\n",
    "                # This is for the sole purpose of not causing any error when reading as csv\n",
    "                # When printing from this column, backticks will be changed back to commas\n",
    "                if i[j+1]==' ':\n",
    "                    _.append('`')\n",
    "                # If it's used in money, (e.g. $3,600) just remove the comma\n",
    "                else:\n",
    "                    _.append('')\n",
    "            else:\n",
    "                _.append(n)\n",
    "        employ_arr.append(\"\".join(_))\n",
    "    employ_arr = np.array(employ_arr)\n",
    "    return employ_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can delete one set of 2019 data since we don't need duplicates from both arr\n",
    "cleaned_17to19 = cleanData(dirtyData_17to19)[:-((len(cleanData(dirtyData_17to19))-1)//3)]\n",
    "\n",
    "# We can also delete the header for the second arr\n",
    "cleaned_19to21 = cleanData(dirtyData_19to21)[1:]\n",
    "\n",
    "cleaned = np.concatenate((cleaned_17to19,cleaned_19to21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: 'datasets_cleaned/employment'\n"
     ]
    }
   ],
   "source": [
    "# Now we can write back the data into datasets_cleaned\n",
    "try:\n",
    "    os.mkdir('datasets_cleaned/employment')\n",
    "except OSError as error:\n",
    "    print(error)  \n",
    "\n",
    "cleaned.tofile('datasets_cleaned/employment/employ.csv',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However at this point our data is still not in the right format yet\n",
    "\n",
    "Due to our need to manipulate and remove commas that were not seperators, \n",
    "we had to cast each row as a string datatype\n",
    "\n",
    "When writing to a csv file, it will cause quotation marks to appear for each row\n",
    "Since we don't want that to affect main.ipynb, we would have to reopen the file the format it\n",
    "\n",
    "This time since we changed the commas, it would be less of a hassle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=np.genfromtxt('datasets_cleaned/employment/employ.csv',dtype='U64',delimiter=',')\n",
    "# Reformat array\n",
    "f = np.char.replace(f,'\\'','')  \n",
    "f = np.char.replace(f,'\\\"','')\n",
    "\n",
    "# We can also remove the '$' and the '%' in the meantime so that we can easily convert into float later\n",
    "f = np.char.replace(f,'$','')\n",
    "f = np.char.replace(f,'%','')\n",
    "\n",
    "\n",
    "# Delete file so that savetxt does not replace chars\n",
    "os.remove('datasets_cleaned/employment/employ.csv')\n",
    "# Write it back to csv in the right format now\n",
    "np.savetxt('datasets_cleaned/employment/employ.csv',f,delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intake\n",
    "Now we repeat the entire process again for the different intakes (_This includes Poly and Uni_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior to this, U64 was used and it accidentally cut off data from Poly Intake\n",
    "dirtyPoly = np.genfromtxt('datasets_src/poly_intake/polytechnics-intake-enrolment-and-graduates-by-course.csv', dtype=\"U128\",delimiter=\"\\n\")\n",
    "dirtyUni = np.genfromtxt('datasets_src/uni_intake/universities-intake-enrolment-and-graduates-by-course.csv',dtype=\"U128\",delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is no salary data, we are just replacing commas with backticks using cleanData\n",
    "cleanPoly = cleanData(dirtyPoly)\n",
    "cleanUni = cleanData(dirtyUni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: 'datasets_cleaned/poly'\n"
     ]
    }
   ],
   "source": [
    "# Poly\n",
    "try:\n",
    "    os.mkdir('datasets_cleaned/poly')\n",
    "except OSError as error:\n",
    "    print(error)  \n",
    "\n",
    "# Generates an error if you already have the file opened elsewhere\n",
    "cleanPoly.tofile('datasets_cleaned/poly/poly_intake.csv',sep='\\n')\n",
    "\n",
    "# Since now each string is one cell of data and not one row, we can go back to using U64\n",
    "f=np.genfromtxt('datasets_cleaned/poly/poly_intake.csv',dtype='U64',delimiter=',')\n",
    "# Reformat array\n",
    "f = np.char.replace(f,'\\'','')  \n",
    "f = np.char.replace(f,'\\\"','')\n",
    "\n",
    "# Delete file so that savetxt does not replace chars\n",
    "os.remove('datasets_cleaned/poly/poly_intake.csv')\n",
    "# Write it back to csv in the right format now\n",
    "np.savetxt('datasets_cleaned/poly/poly_intake.csv',f,delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: 'datasets_cleaned/uni'\n"
     ]
    }
   ],
   "source": [
    "# Uni\n",
    "try:\n",
    "    os.mkdir('datasets_cleaned/uni')\n",
    "except OSError as error:\n",
    "    print(error)  \n",
    "\n",
    "# Generates an error if you already have the file opened elsewhere\n",
    "cleanUni.tofile('datasets_cleaned/uni/uni_intake.csv',sep='\\n')\n",
    "\n",
    "f=np.genfromtxt('datasets_cleaned/uni/uni_intake.csv',dtype='U64',delimiter=',')\n",
    "# Reformat array\n",
    "f = np.char.replace(f,'\\'','')  \n",
    "f = np.char.replace(f,'\\\"','')\n",
    "\n",
    "# Delete file so that savetxt does not replace chars\n",
    "os.remove('datasets_cleaned/uni/uni_intake.csv')\n",
    "# Write it back to csv in the right format now\n",
    "np.savetxt('datasets_cleaned/uni/uni_intake.csv',f,delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "employEdit = np.genfromtxt('datasets_cleaned/employment/employ.csv',dtype='U64',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our employment datasets are from MOE and our intake datasets are from [data.gov.sg](https://www.data.gov.sg/group/education), it is best to rename the column names from both datasets so that it is easier for comparison later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       Initial Col Name   | New Col Name |\n",
    "|----------------------|------|\n",
    "| Engineering | Engineering |\n",
    "| Architecture | Architecture |\n",
    "| Business | Business |\n",
    "| Information & Digital Technologies | IT |\n",
    "| Medicine | Medicine |\n",
    "| Arts, Design & Medias | Arts |\n",
    "| Dentistry | Dentistry |\n",
    "| Built environment | Architecture |\n",
    "|Yale-NUS| *Removed*|\n",
    "|Biomedical Sciences| Health Sciences |\n",
    "| Pharmacy | Health Sciences |\n",
    "| Education (NIE) | Education |\n",
    "| Music | Arts |\n",
    "| Humanities & Social Sciences | Arts |\n",
    "| Health Sciences | Health Sciences |\n",
    "| Sciences | Sciences |\n",
    "| Law | Law |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['year' 'course_cluster' 'employed' 'ft_employment'\n",
      "  'gross median salary']\n",
      " ['2017' 'Arts' '91.40' '65.60' '2944']\n",
      " ['2017' 'Architecture' '92.70' '86.50' '3200']\n",
      " ['2017' 'Business' '95.40' '89.50' '3200']\n",
      " ['2017' 'Dentistry' '100' '100.00' '4050']\n",
      " ['2017' 'Education' '100' '100.00' '3600']\n",
      " ['2017' 'Engineering' '86.70' '79.50' '3500']\n",
      " ['2017' 'Health Sciences' '96.70' '93.70' '3450']\n",
      " ['2017' 'Arts' '85.70' '70.10' '3300']\n",
      " ['2017' 'IT' '94.60' '90.10' '4000']\n",
      " ['2017' 'Arts' '73.30' '26.70' '2225']\n",
      " ['2017' 'Sciences' '82.60' '65.30' '3250']\n",
      " ['2017' 'Architecture' '91.30' '86.40' '4000']\n",
      " ['2017' 'Health Sciences' '92.00' '80.00' '2950']\n",
      " ['2017' 'Law' '96.40' '92.80' '5000']\n",
      " ['2017' 'Medicine' '100.00' '100.00' '5000']\n",
      " ['2017' 'Health Sciences' '99.10' '94.50' '3600']\n",
      " ['2018' 'Arts' '89.10' '68.30' '3000']\n",
      " ['2018' 'Architecture' '91.60' '85.90' '3400']\n",
      " ['2018' 'Business' '94.40' '89.10' '3400']\n",
      " ['2018' 'Dentistry' '100.00' '100.00' '4050']\n",
      " ['2018' 'Education' '100.00' '99.40' '3800']\n",
      " ['2018' 'Engineering' '89.90' '83.80' '3600']\n",
      " ['2018' 'Health Sciences' '96.70' '89.40' '3450']\n",
      " ['2018' 'Arts' '87.30' '72.10' '3400']\n",
      " ['2018' 'IT' '95.00' '92.00' '4022']\n",
      " ['2018' 'Arts' '81.00' '23.80' '1800']\n",
      " ['2018' 'Sciences' '84.40' '68.80' '3313']\n",
      " ['2018' 'Architecture' '96.20' '92.40' '4000']\n",
      " ['2018' 'Health Sciences' '90.90' '81.80' '3000']\n",
      " ['2018' 'Law' '95.30' '91.80' '5000']\n",
      " ['2018' 'Medicine' 'NA' 'NA' 'NA']\n",
      " ['2018' 'Health Sciences' '96.60' '93.20' '3650']\n",
      " ['2019' 'Arts' '87.30' '62.40' '3200']\n",
      " ['2019' 'Architecture' '92.60' '87.30' '3500']\n",
      " ['2019' 'Business' '94.60' '88.80' '3500']\n",
      " ['2019' 'Dentistry' '100' '97.30' '4200']\n",
      " ['2019' 'Education' '100' '100.00' '3800']\n",
      " ['2019' 'Engineering' '88.40' '83.30' '3750']\n",
      " ['2019' 'Health Sciences' '97.10' '88.40' '3500']\n",
      " ['2019' 'Arts' '88.00' '74.50' '3500']\n",
      " ['2019' 'IT' '95.40' '92.70' '4400']\n",
      " ['2019' 'Arts' '82.60' '39.10' '3500']\n",
      " ['2019' 'Sciences' '86.90' '71.50' '3500']\n",
      " ['2019' 'Architecture' '100.00' '96.00' '4100']\n",
      " ['2019' 'Health Sciences' '95.20' '85.70' '3000']\n",
      " ['2019' 'Law' '97.00' '95.00' '5000']\n",
      " ['2019' 'Medicine' '99.60' '99.60' '5300']\n",
      " ['2019' 'Health Sciences' '100' '95.50' '3750']\n",
      " ['2020' 'Arts' '92.30' '50.10' '3300']\n",
      " ['2020' 'Architecture' '93.90' '72.50' '3500']\n",
      " ['2020' 'Business' '95.30' '76.00' '3500']\n",
      " ['2020' 'Dentistry' '100.00' '100.00' '4200']\n",
      " ['2020' 'Education' '100.00' '99.10' '3800']\n",
      " ['2020' 'Engineering' '93.60' '71.60' '3900']\n",
      " ['2020' 'Health Sciences' '97.40' '83.30' '3500']\n",
      " ['2020' 'Arts' '92.10' '61.80' '3500']\n",
      " ['2020' 'IT' '94.80' '87.30' '4760']\n",
      " ['2020' 'Arts' '69.20' '15.40' '3275']\n",
      " ['2020' 'Sciences' '91.60' '55.40' '3500']\n",
      " ['2020' 'Architecture' '94.00' '91.00' '4000']\n",
      " ['2020' 'Health Sciences' '94.70' '84.20' '3000']\n",
      " ['2020' 'Law' '93.40' '88.60' '5000']\n",
      " ['2020' 'Medicine' '100.00' '100.00' '5250']\n",
      " ['2020' 'Health Sciences' '99.30' '93.70' '3700']\n",
      " ['2021' 'Arts' '92.60' '69.30' '3500']\n",
      " ['2021' 'Architecture' '94.50' '88.90' '3600']\n",
      " ['2021' 'Business' '97.00' '88.70' '3723']\n",
      " ['2021' 'Dentistry' '100.00' '100.00' '4200']\n",
      " ['2021' 'Education' '100.00' '100.00' '3798']\n",
      " ['2021' 'Engineering' '94.00' '86.90' '3900']\n",
      " ['2021' 'Health Sciences' '95.70' '85.90' '3635']\n",
      " ['2021' 'Arts' '91.20' '75.60' '3550']\n",
      " ['2021' 'IT' '97.80' '93.70' '5000']\n",
      " ['2021' 'Arts' '82.40' '35.30' '3100']\n",
      " ['2021' 'Sciences' '91.70' '75.80' '3600']\n",
      " ['2021' 'Architecture' '97.70' '96.60' '4000']\n",
      " ['2021' 'Health Sciences' '90.90' '77.30' '3100']\n",
      " ['2021' 'Law' '97.90' '95.50' '5600']\n",
      " ['2021' 'Medicine' 'NA' 'NA' 'NA']\n",
      " ['2021' 'Health Sciences' '95.90' '91.90' '3915']]\n"
     ]
    }
   ],
   "source": [
    "# Rename course name to match the other datasets\n",
    "for a in employEdit:\n",
    "    if a[1]=='Built Environment':\n",
    "        a[1] = 'Architecture'\n",
    "    elif a[1]=='Arts` Design & Media' or a[1]=='Music' or a[1]=='Humanities & Social Sciences':\n",
    "        a[1] = 'Arts'\n",
    "    elif a[1] == 'Information & Digital Technologies':\n",
    "        a[1] = 'IT'\n",
    "    elif a[1]=='Biomedical Sciences' or a[1]=='Pharmacy' or a[1]=='Health Sciences':\n",
    "        a[1] = 'Health Sciences'\n",
    "    elif a[1] == 'Education(NIE)':\n",
    "        a[1] = 'Education'\n",
    "\n",
    "# Remove row if it contains Yale-NUS\n",
    "counter=0\n",
    "for i,a in enumerate(employEdit):\n",
    "    if a[1]=='Yale-NUS':\n",
    "        employEdit = np.delete(employEdit,i-counter,0)\n",
    "        counter+=1\n",
    "\n",
    "print(employEdit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write it back to csv in the right format now\n",
    "np.savetxt('datasets_cleaned/employment/employ.csv',employEdit,delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course cluster: Medicine, year: 2018, column: employed\n",
      "course cluster: Medicine, year: 2018, column: ft_employment\n",
      "course cluster: Medicine, year: 2018, column: gross median salary\n",
      "course cluster: Medicine, year: 2021, column: employed\n",
      "course cluster: Medicine, year: 2021, column: ft_employment\n",
      "course cluster: Medicine, year: 2021, column: gross median salary\n"
     ]
    }
   ],
   "source": [
    "# Read csv file\n",
    "employArr = np.genfromtxt('datasets_cleaned/employment/employ.csv',dtype='float',delimiter=',',skip_header=1,usecols=(0,2,3,4))\n",
    "col_header = np.genfromtxt('datasets_cleaned/employment/employ.csv',dtype='U64',delimiter=',',usecols=(0,2,3,4))[0]\n",
    "employCourse = np.genfromtxt('datasets_cleaned/employment/employ.csv',dtype='U64',delimiter=',',skip_header=1,usecols=(1))\n",
    "# Since we have to use isnan, setting names=True is not an option since the string cannot be set to float\n",
    "# As such we make our own artificial columns using dictionary and 'col_header'\n",
    "employNames = {}\n",
    "for i,n in enumerate(col_header):\n",
    "    employNames[n]=i\n",
    "\n",
    "# Get all missing data\n",
    "missing = np.argwhere(np.isnan(employArr))\n",
    "for i in missing:\n",
    "# Get col name by swapping key and values inside employNames\n",
    "    print(f\"course cluster: {employCourse[i[0]]}, year: {int(employArr[i[0]][0])}, column: {({v:k for k,v in employNames.items()})[i[1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the missing data is for the course cluster \"Medicine\" and for year 2018 and 2021\n",
    "\n",
    "Since:\n",
    "1. The missing data is not randomly distributed (its just for medicine)\n",
    "2. They are accounted for by other data in our datasets (Medicine data of salary and employment percentages from year 2017, 2019 and 2021 are likely to be similar to 2017,2019 and 2020)\n",
    "\n",
    "The missing data is Missing At Random\n",
    "\n",
    "*In the PDF by MOE, it is explained that the missing data is due to insufficient graduates/response rate*\n",
    "\n",
    "Since the data is MAR, we can either choose to impute or remove the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 100.   100.  5000.    99.6   99.6 5300.   100.   100.  5250. ]\n",
      "[2018.     99.87   99.87 5183.33]\n",
      "[2021.     99.87   99.87 5183.33]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Mean imputation\n",
    "\n",
    "# Get data for medicine from 2017,2019 and 2021\n",
    "dataForImpute = np.array([])\n",
    "for i,j in zip(employArr,employCourse):\n",
    "    if i[employNames[\"year\"]] in [2017,2019,2020] and j=='Medicine':\n",
    "        dataForImpute = np.concatenate((dataForImpute,i[1:]))\n",
    "\n",
    "print(dataForImpute)\n",
    "\n",
    "ImputeData = np.zeros((3,3))\n",
    "\n",
    "# Reformat data(by grouping similar cols together) so that np.mean() can be used\n",
    "for iter in range(3):\n",
    "    ImputeData[iter] = np.array([n for i,n in enumerate(dataForImpute) if i%3==iter])\n",
    "\n",
    "# Replace nan values with mean\n",
    "for i,n in enumerate(ImputeData):\n",
    "    employArr[missing[0][0]][i+1] = round(n.mean(),2)\n",
    "    employArr[missing[3][0]][i+1] = round(n.mean(),2)\n",
    "\n",
    "print(employArr[missing[0][0]])\n",
    "print(employArr[missing[3][0]])\n",
    "\n",
    "# If imputation was done correctly, the value of this should be 0\n",
    "print(len(np.argwhere(np.isnan(employArr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['year' 'course_cluster' 'employed' 'ft_employment'\n",
      "  'gross median salary']\n",
      " ['2017.0' 'Arts' '91.4' '65.6' '2944.0']\n",
      " ['2017.0' 'Architecture' '92.7' '86.5' '3200.0']\n",
      " ['2017.0' 'Business' '95.4' '89.5' '3200.0']\n",
      " ['2017.0' 'Dentistry' '100.0' '100.0' '4050.0']\n",
      " ['2017.0' 'Education' '100.0' '100.0' '3600.0']\n",
      " ['2017.0' 'Engineering' '86.7' '79.5' '3500.0']\n",
      " ['2017.0' 'Health Sciences' '96.7' '93.7' '3450.0']\n",
      " ['2017.0' 'Arts' '85.7' '70.1' '3300.0']\n",
      " ['2017.0' 'IT' '94.6' '90.1' '4000.0']\n",
      " ['2017.0' 'Arts' '73.3' '26.7' '2225.0']\n",
      " ['2017.0' 'Sciences' '82.6' '65.3' '3250.0']\n",
      " ['2017.0' 'Architecture' '91.3' '86.4' '4000.0']\n",
      " ['2017.0' 'Health Sciences' '92.0' '80.0' '2950.0']\n",
      " ['2017.0' 'Law' '96.4' '92.8' '5000.0']\n",
      " ['2017.0' 'Medicine' '100.0' '100.0' '5000.0']\n",
      " ['2017.0' 'Health Sciences' '99.1' '94.5' '3600.0']\n",
      " ['2018.0' 'Arts' '89.1' '68.3' '3000.0']\n",
      " ['2018.0' 'Architecture' '91.6' '85.9' '3400.0']\n",
      " ['2018.0' 'Business' '94.4' '89.1' '3400.0']\n",
      " ['2018.0' 'Dentistry' '100.0' '100.0' '4050.0']\n",
      " ['2018.0' 'Education' '100.0' '99.4' '3800.0']\n",
      " ['2018.0' 'Engineering' '89.9' '83.8' '3600.0']\n",
      " ['2018.0' 'Health Sciences' '96.7' '89.4' '3450.0']\n",
      " ['2018.0' 'Arts' '87.3' '72.1' '3400.0']\n",
      " ['2018.0' 'IT' '95.0' '92.0' '4022.0']\n",
      " ['2018.0' 'Arts' '81.0' '23.8' '1800.0']\n",
      " ['2018.0' 'Sciences' '84.4' '68.8' '3313.0']\n",
      " ['2018.0' 'Architecture' '96.2' '92.4' '4000.0']\n",
      " ['2018.0' 'Health Sciences' '90.9' '81.8' '3000.0']\n",
      " ['2018.0' 'Law' '95.3' '91.8' '5000.0']\n",
      " ['2018.0' 'Medicine' '99.87' '99.87' '5183.33']\n",
      " ['2018.0' 'Health Sciences' '96.6' '93.2' '3650.0']\n",
      " ['2019.0' 'Arts' '87.3' '62.4' '3200.0']\n",
      " ['2019.0' 'Architecture' '92.6' '87.3' '3500.0']\n",
      " ['2019.0' 'Business' '94.6' '88.8' '3500.0']\n",
      " ['2019.0' 'Dentistry' '100.0' '97.3' '4200.0']\n",
      " ['2019.0' 'Education' '100.0' '100.0' '3800.0']\n",
      " ['2019.0' 'Engineering' '88.4' '83.3' '3750.0']\n",
      " ['2019.0' 'Health Sciences' '97.1' '88.4' '3500.0']\n",
      " ['2019.0' 'Arts' '88.0' '74.5' '3500.0']\n",
      " ['2019.0' 'IT' '95.4' '92.7' '4400.0']\n",
      " ['2019.0' 'Arts' '82.6' '39.1' '3500.0']\n",
      " ['2019.0' 'Sciences' '86.9' '71.5' '3500.0']\n",
      " ['2019.0' 'Architecture' '100.0' '96.0' '4100.0']\n",
      " ['2019.0' 'Health Sciences' '95.2' '85.7' '3000.0']\n",
      " ['2019.0' 'Law' '97.0' '95.0' '5000.0']\n",
      " ['2019.0' 'Medicine' '99.6' '99.6' '5300.0']\n",
      " ['2019.0' 'Health Sciences' '100.0' '95.5' '3750.0']\n",
      " ['2020.0' 'Arts' '92.3' '50.1' '3300.0']\n",
      " ['2020.0' 'Architecture' '93.9' '72.5' '3500.0']\n",
      " ['2020.0' 'Business' '95.3' '76.0' '3500.0']\n",
      " ['2020.0' 'Dentistry' '100.0' '100.0' '4200.0']\n",
      " ['2020.0' 'Education' '100.0' '99.1' '3800.0']\n",
      " ['2020.0' 'Engineering' '93.6' '71.6' '3900.0']\n",
      " ['2020.0' 'Health Sciences' '97.4' '83.3' '3500.0']\n",
      " ['2020.0' 'Arts' '92.1' '61.8' '3500.0']\n",
      " ['2020.0' 'IT' '94.8' '87.3' '4760.0']\n",
      " ['2020.0' 'Arts' '69.2' '15.4' '3275.0']\n",
      " ['2020.0' 'Sciences' '91.6' '55.4' '3500.0']\n",
      " ['2020.0' 'Architecture' '94.0' '91.0' '4000.0']\n",
      " ['2020.0' 'Health Sciences' '94.7' '84.2' '3000.0']\n",
      " ['2020.0' 'Law' '93.4' '88.6' '5000.0']\n",
      " ['2020.0' 'Medicine' '100.0' '100.0' '5250.0']\n",
      " ['2020.0' 'Health Sciences' '99.3' '93.7' '3700.0']\n",
      " ['2021.0' 'Arts' '92.6' '69.3' '3500.0']\n",
      " ['2021.0' 'Architecture' '94.5' '88.9' '3600.0']\n",
      " ['2021.0' 'Business' '97.0' '88.7' '3723.0']\n",
      " ['2021.0' 'Dentistry' '100.0' '100.0' '4200.0']\n",
      " ['2021.0' 'Education' '100.0' '100.0' '3798.0']\n",
      " ['2021.0' 'Engineering' '94.0' '86.9' '3900.0']\n",
      " ['2021.0' 'Health Sciences' '95.7' '85.9' '3635.0']\n",
      " ['2021.0' 'Arts' '91.2' '75.6' '3550.0']\n",
      " ['2021.0' 'IT' '97.8' '93.7' '5000.0']\n",
      " ['2021.0' 'Arts' '82.4' '35.3' '3100.0']\n",
      " ['2021.0' 'Sciences' '91.7' '75.8' '3600.0']\n",
      " ['2021.0' 'Architecture' '97.7' '96.6' '4000.0']\n",
      " ['2021.0' 'Health Sciences' '90.9' '77.3' '3100.0']\n",
      " ['2021.0' 'Law' '97.9' '95.5' '5600.0']\n",
      " ['2021.0' 'Medicine' '99.87' '99.87' '5183.33']\n",
      " ['2021.0' 'Health Sciences' '95.9' '91.9' '3915.0']]\n"
     ]
    }
   ],
   "source": [
    "# Combine everything back together\n",
    "tmp = np.genfromtxt('datasets_cleaned/employment/employ.csv',dtype='U64',delimiter=',')[0]\n",
    "employ = np.vstack((tmp, np.column_stack((employArr[:,0],employCourse,employArr[:,1:]))))\n",
    "print(employ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "Architecture [2, 12]\n",
      "['2017.0' 'Architecture' '91.3' '86.4' '4000.0'] 1\n",
      "['2017.0' 'Architecture' '92.0' '86.45' '3600.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [1, 8, 10]\n",
      "['2017.0' 'Arts' '85.7' '70.1' '3300.0'] 1\n",
      "['2017.0' 'Arts' '73.3' '26.7' '2225.0'] 2\n",
      "['2017.0' 'Arts' '83.47' '54.13' '2823.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Health Sciences [7, 13, 16]\n",
      "['2017.0' 'Health Sciences' '92.0' '80.0' '2950.0'] 1\n",
      "['2017.0' 'Health Sciences' '99.1' '94.5' '3600.0'] 2\n",
      "['2017.0' 'Health Sciences' '95.93' '89.4' '3333.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Architecture [18, 28]\n",
      "['2018.0' 'Architecture' '96.2' '92.4' '4000.0'] 1\n",
      "['2018.0' 'Architecture' '93.9' '89.15' '3700.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [17, 24, 26]\n",
      "['2018.0' 'Arts' '87.3' '72.1' '3400.0'] 1\n",
      "['2018.0' 'Arts' '81.0' '23.8' '1800.0'] 2\n",
      "['2018.0' 'Arts' '85.8' '54.73' '2733.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Health Sciences [23, 29, 32]\n",
      "['2018.0' 'Health Sciences' '90.9' '81.8' '3000.0'] 1\n",
      "['2018.0' 'Health Sciences' '96.6' '93.2' '3650.0'] 2\n",
      "['2018.0' 'Health Sciences' '94.73' '88.13' '3367.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Architecture [34, 44]\n",
      "['2019.0' 'Architecture' '100.0' '96.0' '4100.0'] 1\n",
      "['2019.0' 'Architecture' '96.3' '91.65' '3800.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [33, 40, 42]\n",
      "['2019.0' 'Arts' '88.0' '74.5' '3500.0'] 1\n",
      "['2019.0' 'Arts' '82.6' '39.1' '3500.0'] 2\n",
      "['2019.0' 'Arts' '85.97' '58.67' '3400.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Health Sciences [39, 45, 48]\n",
      "['2019.0' 'Health Sciences' '95.2' '85.7' '3000.0'] 1\n",
      "['2019.0' 'Health Sciences' '100.0' '95.5' '3750.0'] 2\n",
      "['2019.0' 'Health Sciences' '97.43' '89.87' '3417.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Architecture [50, 60]\n",
      "['2020.0' 'Architecture' '94.0' '91.0' '4000.0'] 1\n",
      "['2020.0' 'Architecture' '93.95' '81.75' '3750.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [49, 56, 58]\n",
      "['2020.0' 'Arts' '92.1' '61.8' '3500.0'] 1\n",
      "['2020.0' 'Arts' '69.2' '15.4' '3275.0'] 2\n",
      "['2020.0' 'Arts' '84.53' '42.43' '3358.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Health Sciences [55, 61, 64]\n",
      "['2020.0' 'Health Sciences' '94.7' '84.2' '3000.0'] 1\n",
      "['2020.0' 'Health Sciences' '99.3' '93.7' '3700.0'] 2\n",
      "['2020.0' 'Health Sciences' '97.13' '87.07' '3400.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Architecture [66, 76]\n",
      "['2021.0' 'Architecture' '97.7' '96.6' '4000.0'] 1\n",
      "['2021.0' 'Architecture' '96.1' '92.75' '3800.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [65, 72, 74]\n",
      "['2021.0' 'Arts' '91.2' '75.6' '3550.0'] 1\n",
      "['2021.0' 'Arts' '82.4' '35.3' '3100.0'] 2\n",
      "['2021.0' 'Arts' '88.73' '60.07' '3383.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Health Sciences [71, 77, 80]\n",
      "['2021.0' 'Health Sciences' '90.9' '77.3' '3100.0'] 1\n",
      "['2021.0' 'Health Sciences' '95.9' '91.9' '3915.0'] 2\n",
      "['2021.0' 'Health Sciences' '94.17' '85.03' '3550.0']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA']\n"
     ]
    }
   ],
   "source": [
    "# Combine intakes,enrollment and graduation data for courses that were group similarly\n",
    "\n",
    "# Start by getting a unique set of all the courses\n",
    "unique_courses = np.unique(employCourse)\n",
    "\n",
    "# Amount of rows we should have at the end (+1 for header)\n",
    "print(len(unique_courses)*(2022-2017))\n",
    "\n",
    "# Add the data to the dictionary\n",
    "for year in range(2017,2022):\n",
    "    \n",
    "    # Create a dictionary to store the data\n",
    "    course_dict = {}\n",
    "    for course in unique_courses:\n",
    "        course_dict[course] = []\n",
    "\n",
    "    for i,a in enumerate(employ):\n",
    "        if a[1] in course_dict and float(a[0])==year:\n",
    "            course_dict[a[1]].append(i)\n",
    "\n",
    "    for k,v in course_dict.items():\n",
    "        if len(v)>1:\n",
    "            print(k,v)\n",
    "            for j,n in enumerate(v):\n",
    "                # Combine the data into one numpy array\n",
    "                if j>0:\n",
    "                    print(employ[n],j)\n",
    "                    employ[v[0]][2] = str(float(employ[v[0]][2])+float(employ[n][2]))\n",
    "                    employ[v[0]][3] = str(float(employ[v[0]][3])+float(employ[n][3]))\n",
    "                    employ[v[0]][4] = str(float(employ[v[0]][4])+float(employ[n][4]))\n",
    "\n",
    "\n",
    "                    # Remove the duplicates later\n",
    "                    employ[n] = np.array(['NA','NA','NA','NA','NA'])\n",
    "            \n",
    "            # Get average\n",
    "            employ[v[0]][2] = str(round(float(employ[v[0]][2])/len(v),2))\n",
    "            employ[v[0]][3] = str(round(float(employ[v[0]][3])/len(v),2))\n",
    "            employ[v[0]][4] = str(round(float(employ[v[0]][4])/len(v),0))\n",
    "            \n",
    "            for n in v:\n",
    "                print(employ[n])\n",
    "    year+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all NA\n",
    "# employ[:,0]!='NA' checks the first col in every row (returns True if not NA)\n",
    "employ = employ[employ[:,0]!='NA']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['year' 'course_cluster' 'employed' 'ft_employment'\n",
      "  'gross median salary']\n",
      " ['2017' 'Arts' '83.47' '54.13' '2823']\n",
      " ['2017' 'Architecture' '92.0' '86.45' '3600']\n",
      " ['2017' 'Business' '95.4' '89.5' '3200']\n",
      " ['2017' 'Dentistry' '100.0' '100.0' '4050']\n",
      " ['2017' 'Education' '100.0' '100.0' '3600']\n",
      " ['2017' 'Engineering' '86.7' '79.5' '3500']\n",
      " ['2017' 'Health Sciences' '95.93' '89.4' '3333']\n",
      " ['2017' 'IT' '94.6' '90.1' '4000']\n",
      " ['2017' 'Sciences' '82.6' '65.3' '3250']\n",
      " ['2017' 'Law' '96.4' '92.8' '5000']\n",
      " ['2017' 'Medicine' '100.0' '100.0' '5000']\n",
      " ['2018' 'Arts' '85.8' '54.73' '2733']\n",
      " ['2018' 'Architecture' '93.9' '89.15' '3700']\n",
      " ['2018' 'Business' '94.4' '89.1' '3400']\n",
      " ['2018' 'Dentistry' '100.0' '100.0' '4050']\n",
      " ['2018' 'Education' '100.0' '99.4' '3800']\n",
      " ['2018' 'Engineering' '89.9' '83.8' '3600']\n",
      " ['2018' 'Health Sciences' '94.73' '88.13' '3367']\n",
      " ['2018' 'IT' '95.0' '92.0' '4022']\n",
      " ['2018' 'Sciences' '84.4' '68.8' '3313']\n",
      " ['2018' 'Law' '95.3' '91.8' '5000']\n",
      " ['2018' 'Medicine' '99.87' '99.87' '5183']\n",
      " ['2019' 'Arts' '85.97' '58.67' '3400']\n",
      " ['2019' 'Architecture' '96.3' '91.65' '3800']\n",
      " ['2019' 'Business' '94.6' '88.8' '3500']\n",
      " ['2019' 'Dentistry' '100.0' '97.3' '4200']\n",
      " ['2019' 'Education' '100.0' '100.0' '3800']\n",
      " ['2019' 'Engineering' '88.4' '83.3' '3750']\n",
      " ['2019' 'Health Sciences' '97.43' '89.87' '3417']\n",
      " ['2019' 'IT' '95.4' '92.7' '4400']\n",
      " ['2019' 'Sciences' '86.9' '71.5' '3500']\n",
      " ['2019' 'Law' '97.0' '95.0' '5000']\n",
      " ['2019' 'Medicine' '99.6' '99.6' '5300']\n",
      " ['2020' 'Arts' '84.53' '42.43' '3358']\n",
      " ['2020' 'Architecture' '93.95' '81.75' '3750']\n",
      " ['2020' 'Business' '95.3' '76.0' '3500']\n",
      " ['2020' 'Dentistry' '100.0' '100.0' '4200']\n",
      " ['2020' 'Education' '100.0' '99.1' '3800']\n",
      " ['2020' 'Engineering' '93.6' '71.6' '3900']\n",
      " ['2020' 'Health Sciences' '97.13' '87.07' '3400']\n",
      " ['2020' 'IT' '94.8' '87.3' '4760']\n",
      " ['2020' 'Sciences' '91.6' '55.4' '3500']\n",
      " ['2020' 'Law' '93.4' '88.6' '5000']\n",
      " ['2020' 'Medicine' '100.0' '100.0' '5250']\n",
      " ['2021' 'Arts' '88.73' '60.07' '3383']\n",
      " ['2021' 'Architecture' '96.1' '92.75' '3800']\n",
      " ['2021' 'Business' '97.0' '88.7' '3723']\n",
      " ['2021' 'Dentistry' '100.0' '100.0' '4200']\n",
      " ['2021' 'Education' '100.0' '100.0' '3798']\n",
      " ['2021' 'Engineering' '94.0' '86.9' '3900']\n",
      " ['2021' 'Health Sciences' '94.17' '85.03' '3550']\n",
      " ['2021' 'IT' '97.8' '93.7' '5000']\n",
      " ['2021' 'Sciences' '91.7' '75.8' '3600']\n",
      " ['2021' 'Law' '97.9' '95.5' '5600']\n",
      " ['2021' 'Medicine' '99.87' '99.87' '5183']]\n"
     ]
    }
   ],
   "source": [
    "# format first and last columns as ints\n",
    "for i,a in enumerate(employ):\n",
    "    if i>0:\n",
    "        employ[i][0] = str(int(float(a[0])))\n",
    "        employ[i][4] = str(int(float(a[4])))\n",
    "print(employ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write back for one last time\n",
    "np.savetxt('datasets_cleaned/employment/employ.csv',employ,delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for intake, we edit the column names to fit column names for Employment dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Poly*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       Initial Col Name   | New Col Name |\n",
    "|----------------------|------|\n",
    "| Engineering Sciences | Engineering |\n",
    "| Architecture, Building & Real Estate | Architecture|\n",
    "| Business & Administration | Business |\n",
    "| Information Technology | IT |\n",
    "| Applied Arts | Arts |\n",
    "| Mass Communication | Arts |\n",
    "| Services | Arts |\n",
    "| Humanities & Social Sciences | Arts |\n",
    "| Health Sciences | Health Sciences |\n",
    "| Education | Education |\n",
    "| Natural & Mathematical Sciences | Sciences |\n",
    "Natural, Physical & Mathematical Sciences | Sciences |\n",
    "| Law | Law |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyEdit = np.genfromtxt('datasets_cleaned/poly/poly_intake.csv',dtype='U64',delimiter=',',skip_header=1)\n",
    "for a in polyEdit:\n",
    "    if 'Engineering' in a[2]:\n",
    "        a[2] = 'Engineering'\n",
    "    elif 'Architecture' in a[2]:\n",
    "        a[2] = 'Architecture'\n",
    "    elif 'Business' in a[2]:\n",
    "        a[2] = 'Business'\n",
    "    elif 'Information Technology' in a[2]:\n",
    "        a[2] = 'IT'\n",
    "    elif a[2] == 'Applied Arts' or a[2] == 'Mass Communication' or a[2]=='Services' or 'Humanities' in a[2]:\n",
    "        a[2] = 'Arts'\n",
    "    elif 'Sciences' in a[2] and a[2]!= 'Health Sciences':\n",
    "        a[2] = 'Sciences'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the dataset, the genders are either:\n",
    "- Both Male and Female or\n",
    "- Female Only\n",
    "\n",
    "To make it easier to compare between different genders, we can take the Both Male and Female row minus the Female Row for the same year and course\n",
    "\n",
    "From 2018 (inclusive) and before, the data is stored as follows:\n",
    "- MF, course1\n",
    "- F, course1\n",
    "- MF, course2\n",
    "- F, course2\n",
    "\n",
    "After 2018, \n",
    "- MF, course1\n",
    "- MF, course2\n",
    "- F, course1\n",
    "- F, course2\n",
    "\n",
    "As such we would need two seperate ways to do this. One for the years for 2018 and prior and another for after 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020' 'M' 'Arts' '70' '211' '89'] 12\n",
      "['2020' 'F' 'Arts' '228' '698' '259'] 13\n",
      "['2020' 'M' 'IT' '1883' '5824' '1808'] 14\n",
      "['2020' 'F' 'IT' '637' '2173' '802'] 15\n",
      "['2020' 'M' 'Law' '48' '118' '31'] 16\n",
      "['2020' 'F' 'Law' '60' '210' '62'] 17\n",
      "['2020' 'M' 'Arts' '134' '458' '159'] 18\n",
      "['2020' 'F' 'Arts' '415' '1334' '461'] 19\n",
      "['2020' 'M' 'Sciences' '440' '1090' '471'] 20\n",
      "['2020' 'F' 'Sciences' '669' '1947' '730'] 21\n",
      "['2020' 'M' 'Arts' '416' '1706' '526'] 22\n",
      "['2020' 'F' 'Arts' '375' '1222' '482'] 23\n"
     ]
    }
   ],
   "source": [
    "for i,a in enumerate(polyEdit):\n",
    "    # 2018 and before\n",
    "    if int(a[0])<=2018:\n",
    "        if a[1]=='MF':\n",
    "            a[1]='M'\n",
    "            a[3] = str(int(a[3])-int(polyEdit[i+1][3]))\n",
    "            a[4] = str(int(a[4])-int(polyEdit[i+1][4]))\n",
    "            a[5] = str(int(a[5])-int(polyEdit[i+1][5]))\n",
    "    # 2019 and 2020\n",
    "    else:\n",
    "        if a[1]=='MF':\n",
    "            a[1]='M'\n",
    "            # Since there are 12 different courses\n",
    "            a[3] = str(int(a[3])-int(polyEdit[i+12][3]))\n",
    "            a[4] = str(int(a[4])-int(polyEdit[i+12][4]))\n",
    "            a[5] = str(int(a[5])-int(polyEdit[i+12][5]))\n",
    "\n",
    "\n",
    "# Reformat data for 2019 and 2020 (to make it similar to 2018 and before)\n",
    "counter = 0\n",
    "for i in range(len(polyEdit)):\n",
    "    if int(polyEdit[i][0])==2019:\n",
    "        if counter<12:\n",
    "            # We have to use vstack instead of concat or append since we are adding a 1d array to a 2d array (and since axis=0)\n",
    "            polyEdit = np.vstack((polyEdit[:i+1+counter],polyEdit[i+12+counter],polyEdit[i+1+counter:]))\n",
    "        # remove duplicates from 2019\n",
    "        elif counter==24:\n",
    "            polyEdit = np.delete(polyEdit,np.s_[i:i+12],0)\n",
    "        counter+=1\n",
    "\n",
    "counter = 0\n",
    "for i in range(len(polyEdit)):\n",
    "    if int(polyEdit[i][0])==2020:\n",
    "        if counter<12:\n",
    "            polyEdit = np.vstack((polyEdit[:i+1+counter],polyEdit[i+12+counter],polyEdit[i+1+counter:]))\n",
    "        else:\n",
    "            print(polyEdit[i],counter)\n",
    "        counter+=1\n",
    "\n",
    "# since the last 12 rows are duplicates, we remove them\n",
    "polyEdit = np.delete(polyEdit,np.s_[-12:],0)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2005' 'M' 'Arts' '441' '1055' '248']\n",
      "['2005' 'F' 'Arts' '687' '1538' '302']\n",
      "['2005' 'M' 'Architecture' '203' '596' '176']\n",
      "['2005' 'F' 'Architecture' '312' '870' '249']\n",
      "['2005' 'M' 'Business' '1094' '3105' '774']\n",
      "['2005' 'F' 'Business' '2389' '7038' '2270']\n",
      "['2005' 'M' 'Education' '9' '15' '0']\n",
      "['2005' 'F' 'Education' '180' '469' '111']\n",
      "['2005' 'M' 'Engineering' '5729' '16523' '4531']\n",
      "['2005' 'F' 'Engineering' '2097' '5939' '2005']\n",
      "['2005' 'M' 'Health Sciences' '313' '991' '139']\n",
      "['2005' 'F' 'Health Sciences' '1326' '3971' '877']\n",
      "['2005' 'M' 'Arts' '10' '10' '0']\n",
      "['2005' 'F' 'Arts' '71' '73' '0']\n",
      "['2005' 'M' 'IT' '2235' '6542' '1892']\n",
      "['2005' 'F' 'IT' '1887' '5065' '1464']\n",
      "['2005' 'M' 'Law' '43' '120' '31']\n",
      "['2005' 'F' 'Law' '83' '221' '71']\n",
      "['2005' 'M' 'Arts' '124' '397' '137']\n",
      "['2005' 'F' 'Arts' '324' '1029' '282']\n",
      "['2005' 'M' 'Sciences' '497' '1166' '321']\n",
      "['2005' 'F' 'Sciences' '712' '1678' '447']\n",
      "['2005' 'M' 'Arts' '72' '296' '153']\n",
      "['2005' 'F' 'Arts' '68' '173' '35']\n",
      "['2006' 'M' 'Arts' '558' '1361' '223']\n",
      "['2006' 'F' 'Arts' '742' '1894' '319']\n",
      "['2006' 'M' 'Architecture' '254' '667' '161']\n",
      "['2006' 'F' 'Architecture' '342' '955' '233']\n",
      "['2006' 'M' 'Business' '1457' '3644' '838']\n",
      "['2006' 'F' 'Business' '2487' '7225' '2208']\n",
      "['2006' 'M' 'Education' '57' '69' '3']\n",
      "['2006' 'F' 'Education' '209' '545' '128']\n",
      "['2006' 'M' 'Engineering' '6101' '17516' '4629']\n",
      "['2006' 'F' 'Engineering' '2108' '6197' '1786']\n",
      "['2006' 'M' 'Health Sciences' '401' '1179' '248']\n",
      "['2006' 'F' 'Health Sciences' '1554' '4428' '1080']\n",
      "['2006' 'M' 'Arts' '9' '17' '0']\n",
      "['2006' 'F' 'Arts' '66' '136' '0']\n",
      "['2006' 'M' 'IT' '2244' '6619' '1874']\n",
      "['2006' 'F' 'IT' '1728' '5134' '1477']\n",
      "['2006' 'M' 'Law' '46' '132' '25']\n",
      "['2006' 'F' 'Law' '70' '221' '63']\n",
      "['2006' 'M' 'Arts' '140' '409' '120']\n",
      "['2006' 'F' 'Arts' '310' '1019' '296']\n",
      "['2006' 'M' 'Sciences' '500' '1286' '301']\n",
      "['2006' 'F' 'Sciences' '684' '1777' '460']\n",
      "['2006' 'M' 'Arts' '115' '321' '116']\n",
      "['2006' 'F' 'Arts' '94' '211' '50']\n",
      "['2007' 'M' 'Arts' '590' '1580' '298']\n",
      "['2007' 'F' 'Arts' '865' '2221' '409']\n",
      "['2007' 'M' 'Architecture' '261' '732' '176']\n",
      "['2007' 'F' 'Architecture' '363' '1010' '273']\n",
      "['2007' 'M' 'Business' '1649' '4259' '996']\n",
      "['2007' 'F' 'Business' '2909' '7839' '2247']\n",
      "['2007' 'M' 'Education' '71' '142' '3']\n",
      "['2007' 'F' 'Education' '238' '623' '156']\n",
      "['2007' 'M' 'Engineering' '5918' '18255' '4622']\n",
      "['2007' 'F' 'Engineering' '2054' '6461' '1685']\n",
      "['2007' 'M' 'Health Sciences' '457' '1263' '355']\n",
      "['2007' 'F' 'Health Sciences' '1653' '4602' '1400']\n",
      "['2007' 'M' 'Arts' '23' '40' '0']\n",
      "['2007' 'F' 'Arts' '88' '222' '0']\n",
      "['2007' 'M' 'IT' '2318' '6889' '1840']\n",
      "['2007' 'F' 'IT' '1651' '5199' '1410']\n",
      "['2007' 'M' 'Law' '36' '125' '38']\n",
      "['2007' 'F' 'Law' '81' '226' '65']\n",
      "['2007' 'M' 'Arts' '176' '439' '135']\n",
      "['2007' 'F' 'Arts' '354' '989' '377']\n",
      "['2007' 'M' 'Sciences' '521' '1452' '311']\n",
      "['2007' 'F' 'Sciences' '846' '2095' '453']\n",
      "['2007' 'M' 'Arts' '154' '380' '113']\n",
      "['2007' 'F' 'Arts' '86' '247' '51']\n",
      "['2008' 'M' 'Arts' '728' '1821' '381']\n",
      "['2008' 'F' 'Arts' '918' '2355' '582']\n",
      "['2008' 'M' 'Architecture' '296' '792' '173']\n",
      "['2008' 'F' 'Architecture' '327' '982' '292']\n",
      "['2008' 'M' 'Business' '2303' '5591' '1061']\n",
      "['2008' 'F' 'Business' '3524' '9221' '2315']\n",
      "['2008' 'M' 'Education' '6' '18' '9']\n",
      "['2008' 'F' 'Education' '188' '529' '175']\n",
      "['2008' 'M' 'Engineering' '5643' '18117' '5161']\n",
      "['2008' 'F' 'Engineering' '1809' '6039' '1996']\n",
      "['2008' 'M' 'Health Sciences' '575' '1421' '348']\n",
      "['2008' 'F' 'Health Sciences' '1788' '5002' '1327']\n",
      "['2008' 'M' 'Arts' '173' '350' '8']\n",
      "['2008' 'F' 'Arts' '306' '603' '67']\n",
      "['2008' 'M' 'IT' '2236' '6930' '2014']\n",
      "['2008' 'F' 'IT' '1707' '5057' '1716']\n",
      "['2008' 'M' 'Law' '62' '130' '43']\n",
      "['2008' 'F' 'Law' '66' '204' '78']\n",
      "['2008' 'M' 'Arts' '164' '503' '117']\n",
      "['2008' 'F' 'Arts' '378' '1036' '318']\n",
      "['2008' 'M' 'Sciences' '521' '1504' '410']\n",
      "['2008' 'F' 'Sciences' '833' '2207' '568']\n",
      "['2008' 'M' 'Arts' '178' '431' '96']\n",
      "['2008' 'F' 'Arts' '109' '294' '62']\n",
      "['2009' 'M' 'Arts' '748' '2052' '428']\n",
      "['2009' 'F' 'Arts' '942' '2629' '576']\n",
      "['2009' 'M' 'Architecture' '254' '788' '233']\n",
      "['2009' 'F' 'Architecture' '291' '936' '307']\n",
      "['2009' 'M' 'Business' '2191' '6077' '1409']\n",
      "['2009' 'F' 'Business' '3524' '9947' '2533']\n",
      "['2009' 'M' 'Education' '16' '29' '4']\n",
      "['2009' 'F' 'Education' '190' '540' '173']\n",
      "['2009' 'M' 'Engineering' '5884' '18326' '5427']\n",
      "['2009' 'F' 'Engineering' '1891' '5922' '1972']\n",
      "['2009' 'M' 'Health Sciences' '586' '1605' '363']\n",
      "['2009' 'F' 'Health Sciences' '1953' '5477' '1396']\n",
      "['2009' 'M' 'Arts' '214' '491' '57']\n",
      "['2009' 'F' 'Arts' '321' '803' '96']\n",
      "['2009' 'M' 'IT' '2237' '6905' '1967']\n",
      "['2009' 'F' 'IT' '1789' '5192' '1505']\n",
      "['2009' 'M' 'Law' '47' '132' '41']\n",
      "['2009' 'F' 'Law' '82' '222' '55']\n",
      "['2009' 'M' 'Arts' '214' '557' '132']\n",
      "['2009' 'F' 'Arts' '471' '1186' '303']\n",
      "['2009' 'M' 'Sciences' '586' '1561' '452']\n",
      "['2009' 'F' 'Sciences' '917' '2416' '592']\n",
      "['2009' 'M' 'Arts' '149' '443' '121']\n",
      "['2009' 'F' 'Arts' '127' '330' '82']\n",
      "['2010' 'M' 'Arts' '865' '2297' '490']\n",
      "['2010' 'F' 'Arts' '1074' '2872' '703']\n",
      "['2010' 'M' 'Architecture' '273' '811' '221']\n",
      "['2010' 'F' 'Architecture' '330' '923' '318']\n",
      "['2010' 'M' 'Business' '2140' '6512' '1582']\n",
      "['2010' 'F' 'Business' '3567' '10455' '2934']\n",
      "['2010' 'M' 'Education' '13' '35' '7']\n",
      "['2010' 'F' 'Education' '258' '625' '167']\n",
      "['2010' 'M' 'Engineering' '5803' '18216' '5402']\n",
      "['2010' 'F' 'Engineering' '1752' '5666' '1893']\n",
      "['2010' 'M' 'Health Sciences' '633' '1809' '377']\n",
      "['2010' 'F' 'Health Sciences' '2022' '5779' '1591']\n",
      "['2010' 'M' 'Arts' '291' '680' '92']\n",
      "['2010' 'F' 'Arts' '392' '993' '183']\n",
      "['2010' 'M' 'IT' '2101' '6768' '2107']\n",
      "['2010' 'F' 'IT' '1586' '5117' '1586']\n",
      "['2010' 'M' 'Law' '58' '152' '34']\n",
      "['2010' 'F' 'Law' '85' '225' '74']\n",
      "['2010' 'M' 'Arts' '179' '593' '123']\n",
      "['2010' 'F' 'Arts' '485' '1347' '254']\n",
      "['2010' 'M' 'Sciences' '541' '1660' '407']\n",
      "['2010' 'F' 'Sciences' '993' '2686' '663']\n",
      "['2010' 'M' 'Arts' '148' '428' '141']\n",
      "['2010' 'F' 'Arts' '118' '340' '96']\n",
      "['2011' 'M' 'Arts' '860' '2503' '573']\n",
      "['2011' 'F' 'Arts' '1130' '3104' '770']\n",
      "['2011' 'M' 'Architecture' '316' '817' '268']\n",
      "['2011' 'F' 'Architecture' '459' '1063' '297']\n",
      "['2011' 'M' 'Business' '2279' '6644' '2035']\n",
      "['2011' 'F' 'Business' '3510' '10571' '3271']\n",
      "['2011' 'M' 'Education' '21' '49' '5']\n",
      "['2011' 'F' 'Education' '255' '687' '178']\n",
      "['2011' 'M' 'Engineering' '6109' '18323' '5372']\n",
      "['2011' 'F' 'Engineering' '1837' '5550' '1814']\n",
      "['2011' 'M' 'Health Sciences' '631' '1868' '497']\n",
      "['2011' 'F' 'Health Sciences' '1906' '5879' '1661']\n",
      "['2011' 'M' 'Arts' '348' '843' '151']\n",
      "['2011' 'F' 'Arts' '422' '1109' '278']\n",
      "['2011' 'M' 'IT' '2316' '6750' '2125']\n",
      "['2011' 'F' 'IT' '1626' '4939' '1607']\n",
      "['2011' 'M' 'Law' '64' '169' '42']\n",
      "['2011' 'F' 'Law' '77' '235' '53']\n",
      "['2011' 'M' 'Arts' '169' '523' '163']\n",
      "['2011' 'F' 'Arts' '418' '1283' '348']\n",
      "['2011' 'M' 'Sciences' '603' '1774' '446']\n",
      "['2011' 'F' 'Sciences' '984' '2846' '732']\n",
      "['2011' 'M' 'Arts' '220' '502' '135']\n",
      "['2011' 'F' 'Arts' '177' '412' '97']\n",
      "['2012' 'M' 'Arts' '860' '2637' '704']\n",
      "['2012' 'F' 'Arts' '1164' '3346' '855']\n",
      "['2012' 'M' 'Architecture' '356' '903' '241']\n",
      "['2012' 'F' 'Architecture' '423' '1185' '271']\n",
      "['2012' 'M' 'Business' '2294' '6743' '2061']\n",
      "['2012' 'F' 'Business' '3772' '10706' '3361']\n",
      "['2012' 'M' 'Education' '19' '53' '16']\n",
      "['2012' 'F' 'Education' '275' '768' '180']\n",
      "['2012' 'M' 'Engineering' '5988' '18236' '5476']\n",
      "['2012' 'F' 'Engineering' '1753' '5301' '1815']\n",
      "['2012' 'M' 'Health Sciences' '568' '1823' '555']\n",
      "['2012' 'F' 'Health Sciences' '1675' '5515' '1923']\n",
      "['2012' 'M' 'Arts' '340' '966' '188']\n",
      "['2012' 'F' 'Arts' '461' '1248' '289']\n",
      "['2012' 'M' 'IT' '2417' '6880' '2062']\n",
      "['2012' 'F' 'IT' '1559' '4797' '1580']\n",
      "['2012' 'M' 'Law' '60' '182' '39']\n",
      "['2012' 'F' 'Law' '79' '232' '75']\n",
      "['2012' 'M' 'Arts' '169' '513' '201']\n",
      "['2012' 'F' 'Arts' '459' '1182' '452']\n",
      "['2012' 'M' 'Sciences' '589' '1764' '546']\n",
      "['2012' 'F' 'Sciences' '1050' '2971' '845']\n",
      "['2012' 'M' 'Arts' '217' '553' '146']\n",
      "['2012' 'F' 'Arts' '207' '499' '118']\n",
      "['2013' 'M' 'Arts' '883' '2546' '783']\n",
      "['2013' 'F' 'Arts' '1214' '3389' '963']\n",
      "['2013' 'M' 'Architecture' '349' '986' '277']\n",
      "['2013' 'F' 'Architecture' '447' '1310' '353']\n",
      "['2013' 'M' 'Business' '2406' '7030' '2035']\n",
      "['2013' 'F' 'Business' '3861' '11087' '3365']\n",
      "['2013' 'M' 'Education' '35' '75' '12']\n",
      "['2013' 'F' 'Education' '288' '805' '250']\n",
      "['2013' 'M' 'Engineering' '5964' '18269' '5398']\n",
      "['2013' 'F' 'Engineering' '1691' '5205' '1618']\n",
      "['2013' 'M' 'Health Sciences' '568' '1749' '601']\n",
      "['2013' 'F' 'Health Sciences' '1842' '5413' '1853']\n",
      "['2013' 'M' 'Arts' '331' '1012' '258']\n",
      "['2013' 'F' 'Arts' '425' '1278' '369']\n",
      "['2013' 'M' 'IT' '2325' '7102' '2014']\n",
      "['2013' 'F' 'IT' '1431' '4520' '1509']\n",
      "['2013' 'M' 'Law' '64' '187' '54']\n",
      "['2013' 'F' 'Law' '76' '218' '78']\n",
      "['2013' 'M' 'Arts' '188' '522' '175']\n",
      "['2013' 'F' 'Arts' '473' '1328' '463']\n",
      "['2013' 'M' 'Sciences' '610' '1785' '547']\n",
      "['2013' 'F' 'Sciences' '992' '2980' '922']\n",
      "['2013' 'M' 'Arts' '214' '588' '143']\n",
      "['2013' 'F' 'Arts' '202' '586' '117']\n",
      "['2014' 'M' 'Arts' '940' '2685' '725']\n",
      "['2014' 'F' 'Arts' '1183' '3448' '995']\n",
      "['2014' 'M' 'Architecture' '326' '1010' '272']\n",
      "['2014' 'F' 'Architecture' '471' '1329' '435']\n",
      "['2014' 'M' 'Business' '2218' '6984' '2202']\n",
      "['2014' 'F' 'Business' '3799' '11355' '3394']\n",
      "['2014' 'M' 'Education' '16' '66' '21']\n",
      "['2014' 'F' 'Education' '317' '872' '239']\n",
      "['2014' 'M' 'Engineering' '5472' '17185' '5371']\n",
      "['2014' 'F' 'Engineering' '1558' '4734' '1612']\n",
      "['2014' 'M' 'Health Sciences' '617' '1748' '575']\n",
      "['2014' 'F' 'Health Sciences' '1659' '5154' '1838']\n",
      "['2014' 'M' 'Arts' '348' '1027' '315']\n",
      "['2014' 'F' 'Arts' '430' '1299' '388']\n",
      "['2014' 'M' 'IT' '2067' '6942' '2094']\n",
      "['2014' 'F' 'IT' '1230' '4204' '1483']\n",
      "['2014' 'M' 'Law' '55' '183' '56']\n",
      "['2014' 'F' 'Law' '79' '224' '66']\n",
      "['2014' 'M' 'Arts' '163' '519' '162']\n",
      "['2014' 'F' 'Arts' '491' '1413' '393']\n",
      "['2014' 'M' 'Sciences' '607' '1804' '555']\n",
      "['2014' 'F' 'Sciences' '1022' '3010' '931']\n",
      "['2014' 'M' 'Arts' '411' '1225' '361']\n",
      "['2014' 'F' 'Arts' '298' '894' '238']\n",
      "['2015' 'M' 'Arts' '718' '2287' '731']\n",
      "['2015' 'F' 'Arts' '1021' '3186' '977']\n",
      "['2015' 'M' 'Architecture' '280' '934' '319']\n",
      "['2015' 'F' 'Architecture' '390' '1283' '396']\n",
      "['2015' 'M' 'Business' '1809' '6231' '2071']\n",
      "['2015' 'F' 'Business' '3062' '10062' '3384']\n",
      "['2015' 'M' 'Education' '21' '66' '18']\n",
      "['2015' 'F' 'Education' '319' '911' '274']\n",
      "['2015' 'M' 'Engineering' '5612' '17117' '5308']\n",
      "['2015' 'F' 'Engineering' '1705' '5064' '1659']\n",
      "['2015' 'M' 'Health Sciences' '694' '2166' '604']\n",
      "['2015' 'F' 'Health Sciences' '1996' '5794' '1693']\n",
      "['2015' 'M' 'Arts' '82' '274' '86']\n",
      "['2015' 'F' 'Arts' '248' '773' '257']\n",
      "['2015' 'M' 'IT' '1963' '6394' '2115']\n",
      "['2015' 'F' 'IT' '1074' '3629' '1298']\n",
      "['2015' 'M' 'Law' '43' '168' '55']\n",
      "['2015' 'F' 'Law' '73' '222' '71']\n",
      "['2015' 'M' 'Arts' '153' '537' '174']\n",
      "['2015' 'F' 'Arts' '456' '1496' '470']\n",
      "['2015' 'M' 'Sciences' '551' '1747' '558']\n",
      "['2015' 'F' 'Sciences' '864' '2817' '919']\n",
      "['2015' 'M' 'Arts' '550' '1959' '611']\n",
      "['2015' 'F' 'Arts' '567' '1748' '583']\n",
      "['2016' 'M' 'Arts' '710' '2172' '759']\n",
      "['2016' 'F' 'Arts' '1011' '3064' '1024']\n",
      "['2016' 'M' 'Architecture' '252' '832' '323']\n",
      "['2016' 'F' 'Architecture' '371' '1207' '420']\n",
      "['2016' 'M' 'Business' '1800' '5797' '2178']\n",
      "['2016' 'F' 'Business' '2846' '9406' '3417']\n",
      "['2016' 'M' 'Education' '38' '71' '30']\n",
      "['2016' 'F' 'Education' '548' '1181' '277']\n",
      "['2016' 'M' 'Engineering' '5416' '16613' '5338']\n",
      "['2016' 'F' 'Engineering' '1395' '4721' '1601']\n",
      "['2016' 'M' 'Health Sciences' '643' '2102' '661']\n",
      "['2016' 'F' 'Health Sciences' '1894' '5704' '1895']\n",
      "['2016' 'M' 'Arts' '86' '262' '92']\n",
      "['2016' 'F' 'Arts' '256' '758' '261']\n",
      "['2016' 'M' 'IT' '1863' '6023' '2030']\n",
      "['2016' 'F' 'IT' '906' '3175' '1275']\n",
      "['2016' 'M' 'Law' '35' '139' '58']\n",
      "['2016' 'F' 'Law' '66' '218' '68']\n",
      "['2016' 'M' 'Arts' '161' '484' '203']\n",
      "['2016' 'F' 'Arts' '457' '1436' '498']\n",
      "['2016' 'M' 'Sciences' '507' '1668' '553']\n",
      "['2016' 'F' 'Sciences' '789' '2634' '904']\n",
      "['2016' 'M' 'Arts' '592' '1858' '668']\n",
      "['2016' 'F' 'Arts' '479' '1624' '571']\n",
      "['2017' 'M' 'Arts' '738' '2163' '682']\n",
      "['2017' 'F' 'Arts' '1147' '3128' '1000']\n",
      "['2017' 'M' 'Architecture' '324' '850' '295']\n",
      "['2017' 'F' 'Architecture' '390' '1129' '441']\n",
      "['2017' 'M' 'Business' '1903' '5617' '2032']\n",
      "['2017' 'F' 'Business' '2931' '8836' '3384']\n",
      "['2017' 'M' 'Education' '59' '115' '13']\n",
      "['2017' 'F' 'Education' '650' '1503' '307']\n",
      "['2017' 'M' 'Engineering' '5457' '16501' '5043']\n",
      "['2017' 'F' 'Engineering' '1481' '4518' '1572']\n",
      "['2017' 'M' 'Health Sciences' '634' '1991' '696']\n",
      "['2017' 'F' 'Health Sciences' '1933' '5807' '1771']\n",
      "['2017' 'M' 'Arts' '93' '256' '90']\n",
      "['2017' 'F' 'Arts' '298' '792' '257']\n",
      "['2017' 'M' 'IT' '2020' '5877' '1990']\n",
      "['2017' 'F' 'IT' '889' '2889' '1099']\n",
      "['2017' 'M' 'Law' '33' '122' '48']\n",
      "['2017' 'F' 'Law' '64' '208' '72']\n",
      "['2017' 'M' 'Arts' '167' '474' '174']\n",
      "['2017' 'F' 'Arts' '475' '1368' '523']\n",
      "['2017' 'M' 'Sciences' '526' '1582' '574']\n",
      "['2017' 'F' 'Sciences' '778' '2400' '955']\n",
      "['2017' 'M' 'Arts' '574' '1751' '645']\n",
      "['2017' 'F' 'Arts' '500' '1559' '547']\n",
      "['2018' 'M' 'Arts' '783' '2209' '669']\n",
      "['2018' 'F' 'Arts' '1111' '3215' '941']\n",
      "['2018' 'M' 'Architecture' '342' '903' '244']\n",
      "['2018' 'F' 'Architecture' '382' '1104' '370']\n",
      "['2018' 'M' 'Business' '1831' '5618' '1778']\n",
      "['2018' 'F' 'Business' '2811' '8546' '3006']\n",
      "['2018' 'M' 'Education' '57' '146' '20']\n",
      "['2018' 'F' 'Education' '660' '1843' '312']\n",
      "['2018' 'M' 'Engineering' '5395' '16317' '4997']\n",
      "['2018' 'F' 'Engineering' '1473' '4328' '1543']\n",
      "['2018' 'M' 'Health Sciences' '665' '1947' '670']\n",
      "['2018' 'F' 'Health Sciences' '2093' '5909' '1895']\n",
      "['2018' 'M' 'Arts' '84' '262' '77']\n",
      "['2018' 'F' 'Arts' '268' '809' '233']\n",
      "['2018' 'M' 'IT' '2003' '5930' '1751']\n",
      "['2018' 'F' 'IT' '817' '2608' '996']\n",
      "['2018' 'M' 'Law' '34' '104' '48']\n",
      "['2018' 'F' 'Law' '70' '197' '76']\n",
      "['2018' 'M' 'Arts' '175' '497' '145']\n",
      "['2018' 'F' 'Arts' '500' '1420' '437']\n",
      "['2018' 'M' 'Sciences' '459' '1486' '528']\n",
      "['2018' 'F' 'Sciences' '773' '2308' '825']\n",
      "['2018' 'M' 'Arts' '647' '1843' '512']\n",
      "['2018' 'F' 'Arts' '436' '1436' '541']\n",
      "['2019' 'M' 'Arts' '692' '2199' '646']\n",
      "['2019' 'F' 'Arts' '1084' '3290' '934']\n",
      "['2019' 'M' 'Architecture' '308' '939' '242']\n",
      "['2019' 'F' 'Architecture' '326' '1052' '337']\n",
      "['2019' 'M' 'Business' '1629' '5432' '1756']\n",
      "['2019' 'F' 'Business' '2659' '8359' '2733']\n",
      "['2019' 'M' 'Education' '57' '165' '33']\n",
      "['2019' 'F' 'Education' '660' '1951' '522']\n",
      "['2019' 'M' 'Engineering' '4953' '15852' '4790']\n",
      "['2019' 'F' 'Engineering' '1409' '4316' '1271']\n",
      "['2019' 'M' 'Health Sciences' '691' '1988' '596']\n",
      "['2019' 'F' 'Health Sciences' '1901' '5904' '1834']\n",
      "['2019' 'M' 'Arts' '66' '242' '83']\n",
      "['2019' 'F' 'Arts' '258' '816' '242']\n",
      "['2019' 'M' 'IT' '1921' '5927' '1705']\n",
      "['2019' 'F' 'IT' '730' '2412' '843']\n",
      "['2019' 'M' 'Law' '36' '104' '35']\n",
      "['2019' 'F' 'Law' '69' '200' '61']\n",
      "['2019' 'M' 'Arts' '153' '490' '154']\n",
      "['2019' 'F' 'Arts' '437' '1396' '447']\n",
      "['2019' 'M' 'Sciences' '445' '1423' '475']\n",
      "['2019' 'F' 'Sciences' '691' '2200' '743']\n",
      "['2019' 'M' 'Arts' '521' '1764' '581']\n",
      "['2019' 'F' 'Arts' '375' '1312' '469']\n",
      "['2020' 'M' 'Arts' '662' '2113' '670']\n",
      "['2020' 'F' 'Arts' '1049' '3199' '1059']\n",
      "['2020' 'M' 'Architecture' '295' '904' '283']\n",
      "['2020' 'F' 'Architecture' '329' '988' '350']\n",
      "['2020' 'M' 'Business' '1635' '5149' '1818']\n",
      "['2020' 'F' 'Business' '2521' '7975' '2785']\n",
      "['2020' 'M' 'Education' '50' '163' '52']\n",
      "['2020' 'F' 'Education' '663' '2001' '661']\n",
      "['2020' 'M' 'Engineering' '4695' '15311' '4964']\n",
      "['2020' 'F' 'Engineering' '1224' '4220' '1363']\n",
      "['2020' 'M' 'Health Sciences' '674' '2031' '586']\n",
      "['2020' 'F' 'Health Sciences' '1842' '5888' '1789']\n",
      "['2020' 'M' 'Arts' '70' '211' '89']\n",
      "['2020' 'F' 'Arts' '228' '698' '259']\n",
      "['2020' 'M' 'IT' '1883' '5824' '1808']\n",
      "['2020' 'F' 'IT' '637' '2173' '802']\n",
      "['2020' 'M' 'Law' '48' '118' '31']\n",
      "['2020' 'F' 'Law' '60' '210' '62']\n",
      "['2020' 'M' 'Arts' '134' '458' '159']\n",
      "['2020' 'F' 'Arts' '415' '1334' '461']\n",
      "['2020' 'M' 'Sciences' '440' '1090' '471']\n",
      "['2020' 'F' 'Sciences' '669' '1947' '730']\n",
      "['2020' 'M' 'Arts' '416' '1706' '526']\n",
      "['2020' 'F' 'Arts' '375' '1222' '482']\n"
     ]
    }
   ],
   "source": [
    "for a in polyEdit:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    }
   ],
   "source": [
    "# Number of rows (+1 for header) we should get\n",
    "# The reason for -2 is because poly has no 'Medicine' and 'Dentistry'\n",
    "print((len(unique_courses)-2)*2*(2021-2005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arts [0, 12, 18, 22]\n",
      "['2005' 'M' 'Arts' '10' '10' '0'] 1\n",
      "['2005' 'M' 'Arts' '124' '397' '137'] 2\n",
      "['2005' 'M' 'Arts' '72' '296' '153'] 3\n",
      "['2005' 'M' 'Arts' '647' '1758' '538']\n",
      "['2005' 'F' 'Arts' '1150' '2813' '619']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [24, 36, 42, 46]\n",
      "['2006' 'M' 'Arts' '9' '17' '0'] 1\n",
      "['2006' 'M' 'Arts' '140' '409' '120'] 2\n",
      "['2006' 'M' 'Arts' '115' '321' '116'] 3\n",
      "['2006' 'M' 'Arts' '822' '2108' '459']\n",
      "['2006' 'F' 'Arts' '1212' '3260' '665']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [48, 60, 66, 70]\n",
      "['2007' 'M' 'Arts' '23' '40' '0'] 1\n",
      "['2007' 'M' 'Arts' '176' '439' '135'] 2\n",
      "['2007' 'M' 'Arts' '154' '380' '113'] 3\n",
      "['2007' 'M' 'Arts' '943' '2439' '546']\n",
      "['2007' 'F' 'Arts' '1393' '3679' '837']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [72, 84, 90, 94]\n",
      "['2008' 'M' 'Arts' '173' '350' '8'] 1\n",
      "['2008' 'M' 'Arts' '164' '503' '117'] 2\n",
      "['2008' 'M' 'Arts' '178' '431' '96'] 3\n",
      "['2008' 'M' 'Arts' '1243' '3105' '602']\n",
      "['2008' 'F' 'Arts' '1711' '4288' '1029']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [96, 108, 114, 118]\n",
      "['2009' 'M' 'Arts' '214' '491' '57'] 1\n",
      "['2009' 'M' 'Arts' '214' '557' '132'] 2\n",
      "['2009' 'M' 'Arts' '149' '443' '121'] 3\n",
      "['2009' 'M' 'Arts' '1325' '3543' '738']\n",
      "['2009' 'F' 'Arts' '1861' '4948' '1057']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [120, 132, 138, 142]\n",
      "['2010' 'M' 'Arts' '291' '680' '92'] 1\n",
      "['2010' 'M' 'Arts' '179' '593' '123'] 2\n",
      "['2010' 'M' 'Arts' '148' '428' '141'] 3\n",
      "['2010' 'M' 'Arts' '1483' '3998' '846']\n",
      "['2010' 'F' 'Arts' '2069' '5552' '1236']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [144, 156, 162, 166]\n",
      "['2011' 'M' 'Arts' '348' '843' '151'] 1\n",
      "['2011' 'M' 'Arts' '169' '523' '163'] 2\n",
      "['2011' 'M' 'Arts' '220' '502' '135'] 3\n",
      "['2011' 'M' 'Arts' '1597' '4371' '1022']\n",
      "['2011' 'F' 'Arts' '2147' '5908' '1493']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [168, 180, 186, 190]\n",
      "['2012' 'M' 'Arts' '340' '966' '188'] 1\n",
      "['2012' 'M' 'Arts' '169' '513' '201'] 2\n",
      "['2012' 'M' 'Arts' '217' '553' '146'] 3\n",
      "['2012' 'M' 'Arts' '1586' '4669' '1239']\n",
      "['2012' 'F' 'Arts' '2291' '6275' '1714']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [192, 204, 210, 214]\n",
      "['2013' 'M' 'Arts' '331' '1012' '258'] 1\n",
      "['2013' 'M' 'Arts' '188' '522' '175'] 2\n",
      "['2013' 'M' 'Arts' '214' '588' '143'] 3\n",
      "['2013' 'M' 'Arts' '1616' '4668' '1359']\n",
      "['2013' 'F' 'Arts' '2314' '6581' '1912']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [216, 228, 234, 238]\n",
      "['2014' 'M' 'Arts' '348' '1027' '315'] 1\n",
      "['2014' 'M' 'Arts' '163' '519' '162'] 2\n",
      "['2014' 'M' 'Arts' '411' '1225' '361'] 3\n",
      "['2014' 'M' 'Arts' '1862' '5456' '1563']\n",
      "['2014' 'F' 'Arts' '2402' '7054' '2014']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [240, 252, 258, 262]\n",
      "['2015' 'M' 'Arts' '82' '274' '86'] 1\n",
      "['2015' 'M' 'Arts' '153' '537' '174'] 2\n",
      "['2015' 'M' 'Arts' '550' '1959' '611'] 3\n",
      "['2015' 'M' 'Arts' '1503' '5057' '1602']\n",
      "['2015' 'F' 'Arts' '2292' '7203' '2287']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [264, 276, 282, 286]\n",
      "['2016' 'M' 'Arts' '86' '262' '92'] 1\n",
      "['2016' 'M' 'Arts' '161' '484' '203'] 2\n",
      "['2016' 'M' 'Arts' '592' '1858' '668'] 3\n",
      "['2016' 'M' 'Arts' '1549' '4776' '1722']\n",
      "['2016' 'F' 'Arts' '2203' '6882' '2354']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [288, 300, 306, 310]\n",
      "['2017' 'M' 'Arts' '93' '256' '90'] 1\n",
      "['2017' 'M' 'Arts' '167' '474' '174'] 2\n",
      "['2017' 'M' 'Arts' '574' '1751' '645'] 3\n",
      "['2017' 'M' 'Arts' '1572' '4644' '1591']\n",
      "['2017' 'F' 'Arts' '2420' '6847' '2327']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [312, 324, 330, 334]\n",
      "['2018' 'M' 'Arts' '84' '262' '77'] 1\n",
      "['2018' 'M' 'Arts' '175' '497' '145'] 2\n",
      "['2018' 'M' 'Arts' '647' '1843' '512'] 3\n",
      "['2018' 'M' 'Arts' '1689' '4811' '1403']\n",
      "['2018' 'F' 'Arts' '2315' '6880' '2152']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [336, 348, 354, 358]\n",
      "['2019' 'M' 'Arts' '66' '242' '83'] 1\n",
      "['2019' 'M' 'Arts' '153' '490' '154'] 2\n",
      "['2019' 'M' 'Arts' '521' '1764' '581'] 3\n",
      "['2019' 'M' 'Arts' '1432' '4695' '1464']\n",
      "['2019' 'F' 'Arts' '2154' '6814' '2092']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [360, 372, 378, 382]\n",
      "['2020' 'M' 'Arts' '70' '211' '89'] 1\n",
      "['2020' 'M' 'Arts' '134' '458' '159'] 2\n",
      "['2020' 'M' 'Arts' '416' '1706' '526'] 3\n",
      "['2020' 'M' 'Arts' '1282' '4488' '1444']\n",
      "['2020' 'F' 'Arts' '2067' '6453' '2261']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n"
     ]
    }
   ],
   "source": [
    "# Add the data to the dictionary\n",
    "for year in range(2005,2021):\n",
    "    \n",
    "    # Create a dictionary to store the data\n",
    "    course_dict = {}\n",
    "    for course in unique_courses:\n",
    "        course_dict[course] = []\n",
    "\n",
    "    for i,a in enumerate(polyEdit):\n",
    "        if a[2] in course_dict and float(a[0])==year:\n",
    "            if a[1]=='M':\n",
    "                course_dict[a[2]].append(i)\n",
    "\n",
    "\n",
    "    for k,v in course_dict.items():\n",
    "        if len(v)>1:\n",
    "            print(k,v)\n",
    "            for j,n in enumerate(v):\n",
    "                # Combine the data into one numpy array\n",
    "                if j>0:\n",
    "                    print(polyEdit[n],j)\n",
    "\n",
    "                    # For males\n",
    "                    polyEdit[v[0]][3] = str(int(polyEdit[v[0]][3])+int(polyEdit[n][3]))\n",
    "                    polyEdit[v[0]][4] = str(int(polyEdit[v[0]][4])+int(polyEdit[n][4]))\n",
    "                    polyEdit[v[0]][5] = str(int(polyEdit[v[0]][5])+int(polyEdit[n][5]))\n",
    "\n",
    "                    # Repeat for Females\n",
    "                    polyEdit[v[0]+1][3] = str(int(polyEdit[v[0]+1][3])+int(polyEdit[n+1][3]))\n",
    "                    polyEdit[v[0]+1][4] = str(int(polyEdit[v[0]+1][4])+int(polyEdit[n+1][4]))\n",
    "                    polyEdit[v[0]+1][5] = str(int(polyEdit[v[0]+1][5])+int(polyEdit[n+1][5]))\n",
    "\n",
    "\n",
    "                    # Remove the duplicates later\n",
    "                    polyEdit[n] = np.array(['NA','NA','NA','NA','NA','NA'])\n",
    "                    polyEdit[n+1] = np.array(['NA','NA','NA','NA','NA','NA'])\n",
    "            \n",
    "            # No need to get average since these are enrollment, intake and grad numbers (not percentages like before)\n",
    "\n",
    "            \n",
    "            for n in v:\n",
    "                print(polyEdit[n])\n",
    "                print(polyEdit[n+1])\n",
    "    year+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['year' 'sex' 'course' 'intake' 'enrolment' 'graduates']\n",
      " ['2005' 'M' 'Arts' '647' '1758' '538']\n",
      " ['2005' 'F' 'Arts' '1150' '2813' '619']\n",
      " ...\n",
      " ['2020' 'F' 'Law' '60' '210' '62']\n",
      " ['2020' 'M' 'Sciences' '440' '1090' '471']\n",
      " ['2020' 'F' 'Sciences' '669' '1947' '730']]\n"
     ]
    }
   ],
   "source": [
    "# Add back header\n",
    "polyEdit = np.vstack((np.genfromtxt('datasets_cleaned/poly/poly_intake.csv',dtype='U64',delimiter=',')[0],polyEdit))\n",
    "# Remove all NA\n",
    "polyEdit = polyEdit[polyEdit[:,0]!='NA']\n",
    "print(polyEdit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write it back to csv in the right format now\n",
    "np.savetxt('datasets_cleaned/poly/poly_intake.csv',polyEdit,delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Uni*\n",
    "\n",
    "Not much explanation to be done since its identical to what was done for the poly datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       Initial Col Name   | New Col Name |\n",
    "|----------------------|------|\n",
    "| Engineering Sciences | Engineering |\n",
    "| Architecture, Building & Real Estate | Architecture|\n",
    "| Business & Administration | Business |\n",
    "| Accountancy | Business |\n",
    "| Information Technology | IT|\n",
    "| Fine & Applied Arts | Arts|\n",
    "| Mass Communication | Arts|\n",
    "| Services | Arts|\n",
    "| Humanities & Social Sciences | Arts|\n",
    "| Medicine | Medicine |\n",
    "| Dentistry | Dentistry |\n",
    "| Health Sciences | Health Sciences |\n",
    "| Education | Education |\n",
    "| Natural & Mathematical Sciences | Sciences |\n",
    "Natural, Physical & Mathematical Sciences | Sciences |\n",
    "| Law | Law |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniEdit = np.genfromtxt('datasets_cleaned/uni/uni_intake.csv',dtype='U64',delimiter=',',skip_header=1)\n",
    "for a in uniEdit:\n",
    "    if 'Engineering' in a[2]:\n",
    "        a[2] = 'Engineering'\n",
    "    elif 'Architecture' in a[2]:\n",
    "        a[2] = 'Architecture'\n",
    "    elif 'Business' in a[2] or a[2] == 'Accountancy':\n",
    "        a[2] = 'Business'\n",
    "    elif 'Information Technology' in a[2]:\n",
    "        a[2] = 'IT'\n",
    "    elif 'Applied Arts' in a[2] or a[2] == 'Mass Communication' or a[2]=='Services' or 'Humanities' in a[2]:\n",
    "        a[2] = 'Arts'\n",
    "    elif 'Sciences' in a[2] and a[2]!= 'Health Sciences':\n",
    "        a[2] = 'Sciences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020' 'F' 'Health Sciences' '761' '2455' '522'] 15\n",
      "['2020' 'M' 'Arts' '929' '4109' '1022'] 16\n",
      "['2020' 'F' 'Arts' '2001' '8783' '2125'] 17\n",
      "['2020' 'M' 'IT' '2017' '6014' '970'] 18\n",
      "['2020' 'F' 'IT' '846' '2797' '397'] 19\n",
      "['2020' 'M' 'Law' '204' '916' '190'] 20\n",
      "['2020' 'F' 'Law' '259' '907' '204'] 21\n",
      "['2020' 'M' 'Arts' '28' '148' '39'] 22\n",
      "['2020' 'F' 'Arts' '142' '566' '142'] 23\n",
      "['2020' 'M' 'Medicine' '253' '1158' '208'] 24\n",
      "['2020' 'F' 'Medicine' '224' '1020' '178'] 25\n",
      "['2020' 'M' 'Sciences' '788' '3089' '801'] 26\n",
      "['2020' 'F' 'Sciences' '891' '3624' '1127'] 27\n",
      "['2020' 'M' 'Arts' '143' '402' '107'] 28\n",
      "['2020' 'F' 'Arts' '139' '412' '112'] 29\n"
     ]
    }
   ],
   "source": [
    "for i,a in enumerate(uniEdit):\n",
    "    if int(a[0])<=2018:\n",
    "        if a[1]=='MF':\n",
    "            a[1]='M'\n",
    "            a[3] = str(int(a[3])-int(uniEdit[i+1][3]))\n",
    "            a[4] = str(int(a[4])-int(uniEdit[i+1][4]))\n",
    "            a[5] = str(int(a[5])-int(uniEdit[i+1][5]))\n",
    "    else:\n",
    "        if a[1]=='MF':\n",
    "            a[1]='M'\n",
    "            # Since there are 12 different courses\n",
    "            a[3] = str(int(a[3])-int(uniEdit[i+15][3]))\n",
    "            a[4] = str(int(a[4])-int(uniEdit[i+15][4]))\n",
    "            a[5] = str(int(a[5])-int(uniEdit[i+15][5]))\n",
    "\n",
    "# Reformat data for 2019 and 2020\n",
    "counter = 0\n",
    "for i in range(len(uniEdit)):\n",
    "    if int(uniEdit[i][0])==2019:\n",
    "        if counter<15:\n",
    "            # We have to use vstack instead of concat or append since we are adding a 1d array to a 2d array (and since axis=0)\n",
    "            uniEdit = np.vstack((uniEdit[:i+1+counter],uniEdit[i+15+counter],uniEdit[i+1+counter:]))\n",
    "        # remove duplicates from 2019\n",
    "        elif counter==30:\n",
    "            uniEdit = np.delete(uniEdit,np.s_[i:i+15],0)\n",
    "        counter+=1\n",
    "\n",
    "counter = 0\n",
    "for i in range(len(uniEdit)):\n",
    "    if int(uniEdit[i][0])==2020:\n",
    "        if counter<15:\n",
    "            uniEdit = np.vstack((uniEdit[:i+1+counter],uniEdit[i+15+counter],uniEdit[i+1+counter:]))\n",
    "        else:\n",
    "            print(uniEdit[i],counter)\n",
    "        counter+=1\n",
    "\n",
    "# since the last 12 rows are duplicates, we remove them\n",
    "uniEdit = np.delete(uniEdit,np.s_[-15:],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n"
     ]
    }
   ],
   "source": [
    "# Number of rows (+1 for header) we should get\n",
    "print((len(unique_courses))*2*(2021-2005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arts [12, 16, 22, 28]\n",
      "['2005' 'M' 'Arts' '589' '1944' '401'] 1\n",
      "['2005' 'M' 'Arts' '42' '137' '36'] 2\n",
      "['2005' 'M' 'Arts' '18' '28' '0'] 3\n",
      "['2005' 'M' 'Arts' '709' '2252' '448']\n",
      "['2005' 'F' 'Arts' '1915' '5598' '1370']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [0, 4]\n",
      "['2005' 'M' 'Business' '644' '1915' '358'] 1\n",
      "['2005' 'M' 'Business' '990' '2744' '569']\n",
      "['2005' 'F' 'Business' '1431' '4830' '1393']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [42, 46, 52, 58]\n",
      "['2006' 'M' 'Arts' '760' '2250' '460'] 1\n",
      "['2006' 'M' 'Arts' '27' '130' '33'] 2\n",
      "['2006' 'M' 'Arts' '32' '60' '0'] 3\n",
      "['2006' 'M' 'Arts' '915' '2663' '504']\n",
      "['2006' 'F' 'Arts' '2139' '6288' '1419']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [30, 34]\n",
      "['2006' 'M' 'Business' '737' '2193' '432'] 1\n",
      "['2006' 'M' 'Business' '1114' '3218' '608']\n",
      "['2006' 'F' 'Business' '1592' '5116' '1286']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [72, 76, 82, 88]\n",
      "['2007' 'M' 'Arts' '756' '2491' '484'] 1\n",
      "['2007' 'M' 'Arts' '46' '147' '28'] 2\n",
      "['2007' 'M' 'Arts' '23' '83' '0'] 3\n",
      "['2007' 'M' 'Arts' '920' '3003' '540']\n",
      "['2007' 'F' 'Arts' '2387' '7314' '1305']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [60, 64]\n",
      "['2007' 'M' 'Business' '664' '2312' '491'] 1\n",
      "['2007' 'M' 'Business' '1039' '3472' '751']\n",
      "['2007' 'F' 'Business' '1694' '5277' '1467']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [102, 106, 112, 118]\n",
      "['2008' 'M' 'Arts' '852' '2826' '505'] 1\n",
      "['2008' 'M' 'Arts' '36' '145' '32'] 2\n",
      "['2008' 'M' 'Arts' '29' '96' '14'] 3\n",
      "['2008' 'M' 'Arts' '1024' '3409' '582']\n",
      "['2008' 'F' 'Arts' '2269' '7986' '1537']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [90, 94]\n",
      "['2008' 'M' 'Business' '593' '2425' '503'] 1\n",
      "['2008' 'M' 'Business' '1010' '3654' '798']\n",
      "['2008' 'F' 'Business' '1581' '5333' '1495']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [132, 136, 142, 148]\n",
      "['2009' 'M' 'Arts' '944' '3162' '573'] 1\n",
      "['2009' 'M' 'Arts' '66' '176' '36'] 2\n",
      "['2009' 'M' 'Arts' '21' '103' '13'] 3\n",
      "['2009' 'M' 'Arts' '1152' '3850' '669']\n",
      "['2009' 'F' 'Arts' '2428' '8496' '1858']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [120, 124]\n",
      "['2009' 'M' 'Business' '695' '2439' '627'] 1\n",
      "['2009' 'M' 'Business' '1174' '3851' '969']\n",
      "['2009' 'F' 'Business' '1487' '5329' '1450']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [162, 166, 172, 178]\n",
      "['2010' 'M' 'Arts' '884' '3319' '736'] 1\n",
      "['2010' 'M' 'Arts' '42' '185' '30'] 2\n",
      "['2010' 'M' 'Arts' '86' '161' '29'] 3\n",
      "['2010' 'M' 'Arts' '1171' '4138' '879']\n",
      "['2010' 'F' 'Arts' '2351' '8884' '1948']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [150, 154]\n",
      "['2010' 'M' 'Business' '706' '2476' '658'] 1\n",
      "['2010' 'M' 'Business' '1186' '3962' '1038']\n",
      "['2010' 'F' 'Business' '1590' '5446' '1409']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [192, 196, 202, 208]\n",
      "['2011' 'M' 'Arts' '793' '3242' '708'] 1\n",
      "['2011' 'M' 'Arts' '39' '183' '41'] 2\n",
      "['2011' 'M' 'Arts' '118' '252' '24'] 3\n",
      "['2011' 'M' 'Arts' '1117' '4216' '857']\n",
      "['2011' 'F' 'Arts' '2244' '8152' '2087']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [180, 184]\n",
      "['2011' 'M' 'Business' '727' '2543' '633'] 1\n",
      "['2011' 'M' 'Business' '1209' '4170' '965']\n",
      "['2011' 'F' 'Business' '1528' '5481' '1451']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [222, 226, 232, 238]\n",
      "['2012' 'M' 'Arts' '838' '3327' '737'] 1\n",
      "['2012' 'M' 'Arts' '37' '184' '37'] 2\n",
      "['2012' 'M' 'Arts' '127' '345' '54'] 3\n",
      "['2012' 'M' 'Arts' '1168' '4442' '936']\n",
      "['2012' 'F' 'Arts' '2392' '8717' '1911']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [210, 214]\n",
      "['2012' 'M' 'Business' '743' '2705' '572'] 1\n",
      "['2012' 'M' 'Business' '1274' '4390' '1019']\n",
      "['2012' 'F' 'Business' '1671' '5684' '1414']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [252, 256, 262, 268]\n",
      "['2013' 'M' 'Arts' '1054' '3550' '818'] 1\n",
      "['2013' 'M' 'Arts' '51' '173' '62'] 2\n",
      "['2013' 'M' 'Arts' '139' '371' '136'] 3\n",
      "['2013' 'M' 'Arts' '1444' '4722' '1154']\n",
      "['2013' 'F' 'Arts' '2716' '9039' '2363']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [240, 244]\n",
      "['2013' 'M' 'Business' '803' '2782' '693'] 1\n",
      "['2013' 'M' 'Business' '1309' '4550' '1092']\n",
      "['2013' 'F' 'Business' '1656' '5680' '1585']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [282, 286, 292, 298]\n",
      "['2014' 'M' 'Arts' '1022' '3756' '803'] 1\n",
      "['2014' 'M' 'Arts' '46' '174' '44'] 2\n",
      "['2014' 'M' 'Arts' '54' '286' '135'] 3\n",
      "['2014' 'M' 'Arts' '1309' '4857' '1147']\n",
      "['2014' 'F' 'Arts' '2619' '9420' '2181']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [270, 274]\n",
      "['2014' 'M' 'Business' '837' '2980' '631'] 1\n",
      "['2014' 'M' 'Business' '1492' '4892' '1104']\n",
      "['2014' 'F' 'Business' '1925' '6053' '1495']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [312, 316, 322, 328]\n",
      "['2015' 'M' 'Arts' '1033' '3861' '743'] 1\n",
      "['2015' 'M' 'Arts' '40' '174' '42'] 2\n",
      "['2015' 'M' 'Arts' '105' '352' '154'] 3\n",
      "['2015' 'M' 'Arts' '1400' '5052' '1126']\n",
      "['2015' 'F' 'Arts' '2677' '9963' '2001']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [300, 304]\n",
      "['2015' 'M' 'Business' '848' '3111' '771'] 1\n",
      "['2015' 'M' 'Business' '1488' '5150' '1251']\n",
      "['2015' 'F' 'Business' '2102' '6711' '1404']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [342, 346, 352, 358]\n",
      "['2016' 'M' 'Arts' '1014' '4046' '803'] 1\n",
      "['2016' 'M' 'Arts' '42' '178' '35'] 2\n",
      "['2016' 'M' 'Arts' '91' '355' '84'] 3\n",
      "['2016' 'M' 'Arts' '1308' '5249' '1095']\n",
      "['2016' 'F' 'Arts' '2760' '10385' '2267']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [330, 334]\n",
      "['2016' 'M' 'Business' '959' '3284' '751'] 1\n",
      "['2016' 'M' 'Business' '1568' '5465' '1190']\n",
      "['2016' 'F' 'Business' '2149' '7139' '1633']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [372, 376, 382, 388]\n",
      "['2017' 'M' 'Arts' '1105' '4185' '945'] 1\n",
      "['2017' 'M' 'Arts' '54' '182' '48'] 2\n",
      "['2017' 'M' 'Arts' '81' '335' '99'] 3\n",
      "['2017' 'M' 'Arts' '1428' '5374' '1259']\n",
      "['2017' 'F' 'Arts' '2600' '10408' '2465']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [360, 364]\n",
      "['2017' 'M' 'Business' '988' '3500' '755'] 1\n",
      "['2017' 'M' 'Business' '1662' '5763' '1308']\n",
      "['2017' 'F' 'Business' '2239' '7628' '1661']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [402, 406, 412, 418]\n",
      "['2018' 'M' 'Arts' '1041' '4220' '945'] 1\n",
      "['2018' 'M' 'Arts' '42' '178' '46'] 2\n",
      "['2018' 'M' 'Arts' '118' '353' '98'] 3\n",
      "['2018' 'M' 'Arts' '1364' '5413' '1269']\n",
      "['2018' 'F' 'Arts' '2824' '10683' '2491']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [390, 394]\n",
      "['2018' 'M' 'Business' '1084' '3741' '837'] 1\n",
      "['2018' 'M' 'Business' '1774' '6062' '1416']\n",
      "['2018' 'F' 'Business' '2432' '8175' '1767']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [432, 436, 442, 448]\n",
      "['2019' 'M' 'Arts' '1012' '4227' '978'] 1\n",
      "['2019' 'M' 'Arts' '24' '161' '41'] 2\n",
      "['2019' 'M' 'Arts' '116' '363' '95'] 3\n",
      "['2019' 'M' 'Arts' '1326' '5388' '1295']\n",
      "['2019' 'F' 'Arts' '2794' '10862' '2551']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [420, 424]\n",
      "['2019' 'M' 'Business' '1251' '4134' '847'] 1\n",
      "['2019' 'M' 'Business' '1904' '6461' '1435']\n",
      "['2019' 'F' 'Business' '2639' '8695' '2008']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Arts [462, 466, 472, 478]\n",
      "['2020' 'M' 'Arts' '929' '4109' '1022'] 1\n",
      "['2020' 'M' 'Arts' '28' '148' '39'] 2\n",
      "['2020' 'M' 'Arts' '143' '402' '107'] 3\n",
      "['2020' 'M' 'Arts' '1233' '5265' '1325']\n",
      "['2020' 'F' 'Arts' '2533' '10672' '2651']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "Business [450, 454]\n",
      "['2020' 'M' 'Business' '1339' '4591' '896'] 1\n",
      "['2020' 'M' 'Business' '1895' '6814' '1495']\n",
      "['2020' 'F' 'Business' '2851' '9443' '2021']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n",
      "['NA' 'NA' 'NA' 'NA' 'NA' 'NA']\n"
     ]
    }
   ],
   "source": [
    "# Add the data to the dictionary\n",
    "for year in range(2005,2021):\n",
    "    \n",
    "    # Create a dictionary to store the data\n",
    "    course_dict = {}\n",
    "    for course in unique_courses:\n",
    "        course_dict[course] = []\n",
    "\n",
    "    for i,a in enumerate(uniEdit):\n",
    "        if a[2] in course_dict and float(a[0])==year:\n",
    "            if a[1]=='M':\n",
    "                course_dict[a[2]].append(i)\n",
    "\n",
    "\n",
    "    for k,v in course_dict.items():\n",
    "        if len(v)>1:\n",
    "            print(k,v)\n",
    "            for j,n in enumerate(v):\n",
    "                # Combine the data into one numpy array\n",
    "                if j>0:\n",
    "                    print(uniEdit[n],j)\n",
    "\n",
    "                    # For males\n",
    "                    uniEdit[v[0]][3] = str(int(uniEdit[v[0]][3])+int(uniEdit[n][3]))\n",
    "                    uniEdit[v[0]][4] = str(int(uniEdit[v[0]][4])+int(uniEdit[n][4]))\n",
    "                    uniEdit[v[0]][5] = str(int(uniEdit[v[0]][5])+int(uniEdit[n][5]))\n",
    "\n",
    "                    # Repeat for Females\n",
    "                    uniEdit[v[0]+1][3] = str(int(uniEdit[v[0]+1][3])+int(uniEdit[n+1][3]))\n",
    "                    uniEdit[v[0]+1][4] = str(int(uniEdit[v[0]+1][4])+int(uniEdit[n+1][4]))\n",
    "                    uniEdit[v[0]+1][5] = str(int(uniEdit[v[0]+1][5])+int(uniEdit[n+1][5]))\n",
    "\n",
    "\n",
    "                    # Remove the duplicates later\n",
    "                    uniEdit[n] = np.array(['NA','NA','NA','NA','NA','NA'])\n",
    "                    uniEdit[n+1] = np.array(['NA','NA','NA','NA','NA','NA'])\n",
    "            \n",
    "            # No need to get average since these are enrollment, intake and grad numbers (not percentages like before)\n",
    "\n",
    "            \n",
    "            for n in v:\n",
    "                print(uniEdit[n])\n",
    "                print(uniEdit[n+1])\n",
    "    year+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['year' 'sex' 'course' 'intake' 'enrolment' 'graduates']\n",
      " ['2005' 'M' 'Business' '990' '2744' '569']\n",
      " ['2005' 'F' 'Business' '1431' '4830' '1393']\n",
      " ...\n",
      " ['2020' 'F' 'Medicine' '224' '1020' '178']\n",
      " ['2020' 'M' 'Sciences' '788' '3089' '801']\n",
      " ['2020' 'F' 'Sciences' '891' '3624' '1127']]\n"
     ]
    }
   ],
   "source": [
    "# Add back header\n",
    "uniEdit = np.vstack((np.genfromtxt('datasets_cleaned/uni/uni_intake.csv',dtype='U64',delimiter=',')[0],uniEdit))\n",
    "# Remove all NA\n",
    "uniEdit = uniEdit[uniEdit[:,0]!='NA']\n",
    "print(uniEdit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write it back to csv in the right format now\n",
    "np.savetxt('datasets_cleaned/uni/uni_intake.csv',uniEdit,delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b40f880619167c1897eb19ed28404e44ef5681baa2b94c6a6ce5dad7d6c8be5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
