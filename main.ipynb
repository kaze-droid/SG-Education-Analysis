{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done By: **Ryan Yeo**\n",
    "\n",
    "Class: **DAAA/FT/1B/01**\n",
    "\n",
    "Admin Num: **P2214452**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it exists print err\n",
    "try:\n",
    "    os.mkdir('datasets_cleaned')\n",
    "except OSError as error:\n",
    "    print(error)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean employment dataset\n",
    "\n",
    "# Note:\n",
    "# We cannot use genfromtxt directly because it reads commas contained in strings(unlike csvreader and pd.read_csv) \n",
    "# To avoid that, we first read in all the data seperated by a newline before processing it\n",
    "\n",
    "dirtyData_17to19 = np.genfromtxt('datasets_src/employment/emp_17to19.csv', dtype=\"U64\",delimiter=\"\\n\")\n",
    "dirtyData_19to21 = np.genfromtxt('datasets_src/employment/emp_19to21.csv', dtype=\"U64\",delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert and clean the dataset so that it can be written to datasets_cleaned\n",
    "def cleanData(dirtyArr):\n",
    "    employ_arr = []\n",
    "    for i in dirtyArr:\n",
    "        _ = []\n",
    "        inQuotes = False\n",
    "        for j,n in enumerate(i):\n",
    "            if n=='\\\"':\n",
    "                # If opening quotes => True elif closing quotes => False\n",
    "                inQuotes=not inQuotes\n",
    "            if n==',' and inQuotes:\n",
    "                # If it's used in a string, change it to a backtick\n",
    "                # This is for the sole purpose of not causing any error when reading as csv\n",
    "                # When printing from this column, backticks will be changed back to commas\n",
    "                if i[j+1]==' ':\n",
    "                    _.append('`')\n",
    "                # If it's used in money, (e.g. $3,600) just remove the comma\n",
    "                else:\n",
    "                    _.append('')\n",
    "            else:\n",
    "                _.append(n)\n",
    "        employ_arr.append(\"\".join(_))\n",
    "    employ_arr = np.array(employ_arr)\n",
    "    return employ_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can delete one set of 2019 data since we don't need duplicates from both arr\n",
    "cleaned_17to19 = cleanData(dirtyData_17to19)[:-((len(cleanData(dirtyData_17to19))-1)//3)]\n",
    "\n",
    "# We can also delete the header for the second arr\n",
    "cleaned_19to21 = cleanData(dirtyData_19to21)[1:]\n",
    "\n",
    "cleaned = np.concatenate((cleaned_17to19,cleaned_19to21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can write back the data into datasets_cleaned\n",
    "try:\n",
    "    os.mkdir('datasets_cleaned/employment')\n",
    "except OSError as error:\n",
    "    print(error)  \n",
    "\n",
    "cleaned.tofile('datasets_cleaned/employment/employ.csv',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However at this point our data is still not in the right format yet\n",
    "\n",
    "Due to our need to manipulate and remove commas that were not seperators, \n",
    "we had to cast each row as a string datatype\n",
    "\n",
    "When writing to a csv file, it will cause quotation marks to appear for each row\n",
    "Since we don't want that to affect main.ipynb, we would have to reopen the file the format it\n",
    "\n",
    "This time since we changed the commas, it would be less of a hassle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=np.genfromtxt('datasets_cleaned/employment/employ.csv',dtype='U64',delimiter=',')\n",
    "# Reformat array\n",
    "f = np.char.replace(f,'\\'','')  \n",
    "f = np.char.replace(f,'\\\"','')\n",
    "\n",
    "# We can also remove the '$' and the '%' in the meantime so that we can easily convert into float later\n",
    "f = np.char.replace(f,'$','')\n",
    "f = np.char.replace(f,'%','')\n",
    "\n",
    "\n",
    "# Delete file so that savetxt does not replace chars\n",
    "os.remove('datasets_cleaned/employment/employ.csv')\n",
    "# Write it back to csv in the right format now\n",
    "np.savetxt('datasets_cleaned/employment/employ.csv',f,delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intake\n",
    "Now we repeat the entire process again for the different intakes (_This includes ITE, poly and Uni_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior to this, U64 was used and it accidentally cut off data from Poly Intake\n",
    "dirtyITE = np.genfromtxt('datasets_src/ite_intake/intake-count-of-full-time-and-traineeship-programmes-at-ite.csv', dtype=\"U128\",delimiter=\"\\n\")\n",
    "dirtyPoly = np.genfromtxt('datasets_src/poly_intake/polytechnics-intake-enrolment-and-graduates-by-course.csv', dtype=\"U128\",delimiter=\"\\n\")\n",
    "dirtyUni = np.genfromtxt('datasets_src/uni_intake/universities-intake-enrolment-and-graduates-by-course.csv',dtype=\"U128\",delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is no salary data, we are just replacing commas with backticks using cleanData\n",
    "cleanITE = cleanData(dirtyITE)\n",
    "cleanPoly = cleanData(dirtyPoly)\n",
    "cleanUni = cleanData(dirtyUni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: 'datasets_cleaned/ite'\n"
     ]
    }
   ],
   "source": [
    "# ITE\n",
    "try:\n",
    "    os.mkdir('datasets_cleaned/ite')\n",
    "except OSError as error:\n",
    "    print(error)  \n",
    "\n",
    "cleanITE.tofile('datasets_cleaned/ite/ite_intake.csv',sep='\\n')\n",
    "\n",
    "f=np.genfromtxt('datasets_cleaned/ite/ite_intake.csv',dtype='U64',delimiter=',')\n",
    "# Reformat array\n",
    "f = np.char.replace(f,'\\'','')  \n",
    "f = np.char.replace(f,'\\\"','')\n",
    "\n",
    "# Delete file so that savetxt does not replace chars\n",
    "os.remove('datasets_cleaned/ite/ite_intake.csv')\n",
    "# Write it back to csv in the right format now\n",
    "np.savetxt('datasets_cleaned/ite/ite_intake.csv',f,delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: 'datasets_cleaned/poly'\n"
     ]
    }
   ],
   "source": [
    "# Poly\n",
    "try:\n",
    "    os.mkdir('datasets_cleaned/poly')\n",
    "except OSError as error:\n",
    "    print(error)  \n",
    "\n",
    "cleanPoly.tofile('datasets_cleaned/poly/poly_intake.csv',sep='\\n')\n",
    "\n",
    "# Since now each string is one cell of data and not one row, we can go back to using U64\n",
    "f=np.genfromtxt('datasets_cleaned/poly/poly_intake.csv',dtype='U64',delimiter=',')\n",
    "# Reformat array\n",
    "f = np.char.replace(f,'\\'','')  \n",
    "f = np.char.replace(f,'\\\"','')\n",
    "\n",
    "# Delete file so that savetxt does not replace chars\n",
    "os.remove('datasets_cleaned/poly/poly_intake.csv')\n",
    "# Write it back to csv in the right format now\n",
    "np.savetxt('datasets_cleaned/poly/poly_intake.csv',f,delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uni\n",
    "try:\n",
    "    os.mkdir('datasets_cleaned/uni')\n",
    "except OSError as error:\n",
    "    print(error)  \n",
    "\n",
    "cleanUni.tofile('datasets_cleaned/uni/uni_intake.csv',sep='\\n')\n",
    "\n",
    "f=np.genfromtxt('datasets_cleaned/uni/uni_intake.csv',dtype='U64',delimiter=',')\n",
    "# Reformat array\n",
    "f = np.char.replace(f,'\\'','')  \n",
    "f = np.char.replace(f,'\\\"','')\n",
    "\n",
    "# Delete file so that savetxt does not replace chars\n",
    "os.remove('datasets_cleaned/uni/uni_intake.csv')\n",
    "# Write it back to csv in the right format now\n",
    "np.savetxt('datasets_cleaned/uni/uni_intake.csv',f,delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Categorise data based on course_clusters\n",
    "# Combine all three intakes into 1\n",
    "# For ITE, use Z score to compare between ITE diploma, Higher NITEC etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course cluster: Medicine, year: 2018, column: 1\n",
      "course cluster: Medicine, year: 2018, column: 2\n",
      "course cluster: Medicine, year: 2018, column: 3\n",
      "course cluster: Medicine, year: 2021, column: 1\n",
      "course cluster: Medicine, year: 2021, column: 2\n",
      "course cluster: Medicine, year: 2021, column: 3\n"
     ]
    }
   ],
   "source": [
    "# Read csv file\n",
    "employArr = np.genfromtxt('datasets_cleaned/employment/employ.csv',dtype='float',delimiter=',',skip_header=1,usecols=(0,2,3,4))\n",
    "label = np.genfromtxt('datasets_cleaned/employment/employ.csv',dtype='U64',delimiter=',',skip_header=1,usecols=(1))\n",
    "\n",
    "# Get all missing data\n",
    "missing = np.argwhere(np.isnan(employArr))\n",
    "for i in missing:\n",
    "    print(f\"course cluster: {label[i[0]]}, year: {int(employArr[i[0]][0])}, column: {i[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the missing data is for the course cluster \"Medicine\" and for year 2018 and 2021\n",
    "\n",
    "Since:\n",
    "1. The missing data is not randomly distributed (its just for medicine)\n",
    "2. They are accounted for by other data in our datasets (Medicine data of salary and employment percentages from year 2017, 2019 and 2021 are likely to be similar to 2017,2019 and 2020)\n",
    "\n",
    "The missing data is Missing At Random\n",
    "\n",
    "*In the PDF by MOE, it is explained that the missing data is due to insufficient graduates/response rate*\n",
    "\n",
    "Since the data is MAR, we can either choose to impute or remove the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 100.   100.  5000.    99.6   99.6 5300.   100.   100.  5250. ]\n",
      "[2018.     99.87   99.87 5183.33]\n",
      "[2021.     99.87   99.87 5183.33]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Mean imputation\n",
    "\n",
    "# Get data for medicine from 2017,2019 and 2021\n",
    "dataForImpute = np.array([])\n",
    "for i,j in zip(employArr,label):\n",
    "    if i[0] in [2017,2019,2020] and j=='Medicine':\n",
    "        dataForImpute = np.concatenate((dataForImpute,i[1:]))\n",
    "\n",
    "print(dataForImpute)\n",
    "\n",
    "ImputeData = np.zeros((3,3))\n",
    "\n",
    "# Reformat data(by grouping similar cols together) so that np.mean() can be used\n",
    "for iter in range(3):\n",
    "    ImputeData[iter] = np.array([n for i,n in enumerate(dataForImpute) if i%3==iter])\n",
    "\n",
    "# Replace nan values with mean\n",
    "for i,n in enumerate(ImputeData):\n",
    "    employArr[missing[0][0]][i+1] = round(n.mean(),2)\n",
    "    employArr[missing[3][0]][i+1] = round(n.mean(),2)\n",
    "\n",
    "print(employArr[missing[0][0]])\n",
    "print(employArr[missing[3][0]])\n",
    "\n",
    "# If imputation was done correctly, the value of this should be 0\n",
    "print(len(np.argwhere(np.isnan(employArr))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b40f880619167c1897eb19ed28404e44ef5681baa2b94c6a6ce5dad7d6c8be5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
